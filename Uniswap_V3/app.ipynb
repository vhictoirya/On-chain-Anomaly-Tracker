{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e32d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, HTTPException, BackgroundTasks\n",
    "from pydantic import BaseModel\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import time\n",
    "from typing import Dict, List, Optional, Set\n",
    "from collections import defaultdict, deque\n",
    "import statistics\n",
    "from datetime import datetime, timedelta\n",
    "import logging\n",
    "\n",
    "app = FastAPI(title=\"Onchain Anomaly Tracker\")\n",
    "\n",
    "# Configuration\n",
    "ETHERSCAN_API_KEY = \"YOUR_ETHERSCAN_API_KEY\"\n",
    "WEBACY_API_KEY = \"YOUR_WEBACY_API_KEY\"\n",
    "RATE_LIMIT_DELAY = 0.2  # seconds between API calls\n",
    "\n",
    "# Global caches and state\n",
    "contract_cache = {}\n",
    "address_monitor = {}  # Track addresses being monitored\n",
    "anomaly_cache = deque(maxlen=10000)  # Store recent anomalies\n",
    "\n",
    "class AddressAnalysis(BaseModel):\n",
    "    address: str\n",
    "    address_type: str\n",
    "    anomaly_score: int\n",
    "    risk_level: str\n",
    "    flags: List[str]\n",
    "    webacy_sanctioned: bool\n",
    "    webacy_threat_score: Optional[float] = None\n",
    "\n",
    "class AnomalyDetector:\n",
    "    def __init__(self):\n",
    "        self.session = None\n",
    "        self.monitored_addresses: Set[str] = set()\n",
    "        \n",
    "    async def init_session(self):\n",
    "        if not self.session:\n",
    "            self.session = aiohttp.ClientSession()\n",
    "    \n",
    "    async def close_session(self):\n",
    "        if self.session:\n",
    "            await self.session.close()\n",
    "\n",
    "    async def is_contract(self, address: str) -> bool:\n",
    "        \"\"\"Cached contract detection\"\"\"\n",
    "        if address in contract_cache:\n",
    "            return contract_cache[address]\n",
    "        \n",
    "        await self.init_session()\n",
    "        url = f\"https://api.etherscan.io/api?module=proxy&action=eth_getCode&address={address}&tag=latest&apikey={ETHERSCAN_API_KEY}\"\n",
    "        \n",
    "        try:\n",
    "            async with self.session.get(url) as response:\n",
    "                data = await response.json()\n",
    "                result = data.get(\"result\", \"0x\") != \"0x\"\n",
    "                contract_cache[address] = result\n",
    "                await asyncio.sleep(RATE_LIMIT_DELAY)\n",
    "                return result\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error checking contract status for {address}: {e}\")\n",
    "            return False\n",
    "\n",
    "    async def fetch_transactions_batch(self, address: str, action: str = \"txlist\", max_pages: int = 2) -> List[Dict]:\n",
    "        \"\"\"Optimized transaction fetching with recent data focus\"\"\"\n",
    "        await self.init_session()\n",
    "        transactions = []\n",
    "        \n",
    "        # Get only recent transactions (last 1000 blocks ~4 hours)\n",
    "        latest_block = await self.get_latest_block()\n",
    "        start_block = max(0, latest_block - 1000)\n",
    "        \n",
    "        for page in range(1, max_pages + 1):\n",
    "            url = f\"https://api.etherscan.io/v2/api?chainid=1&module=account&action={action}&address={address}&startblock={start_block}&endblock=latest&page={page}&offset=1000&sort=desc&apikey={ETHERSCAN_API_KEY}\"\n",
    "            \n",
    "            try:\n",
    "                async with self.session.get(url) as response:\n",
    "                    data = await response.json()\n",
    "                    txs = data.get(\"result\", [])\n",
    "                    \n",
    "                    if not txs or txs == \"No transactions found\":\n",
    "                        break\n",
    "                        \n",
    "                    transactions.extend(txs)\n",
    "                    await asyncio.sleep(RATE_LIMIT_DELAY)\n",
    "                    \n",
    "                    # If we get less than 1000, we've got all recent data\n",
    "                    if len(txs) < 1000:\n",
    "                        break\n",
    "                        \n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error fetching {action} for {address}: {e}\")\n",
    "                break\n",
    "        \n",
    "        return transactions\n",
    "\n",
    "    async def get_latest_block(self) -> int:\n",
    "        \"\"\"Get current block number\"\"\"\n",
    "        await self.init_session()\n",
    "        url = f\"https://api.etherscan.io/api?module=proxy&action=eth_blockNumber&apikey={ETHERSCAN_API_KEY}\"\n",
    "        \n",
    "        try:\n",
    "            async with self.session.get(url) as response:\n",
    "                data = await response.json()\n",
    "                return int(data.get(\"result\", \"0x0\"), 16)\n",
    "        except:\n",
    "            return 0\n",
    "\n",
    "    def analyze_transaction_frequency(self, txs: List[Dict], window_minutes: int = 10) -> Dict:\n",
    "        \"\"\"Detect unusual transaction frequency patterns\"\"\"\n",
    "        if not txs:\n",
    "            return {\"frequency_anomaly\": False}\n",
    "            \n",
    "        timestamps = [int(tx['timeStamp']) for tx in txs]\n",
    "        timestamps.sort()\n",
    "        \n",
    "        # Group by time windows\n",
    "        window_seconds = window_minutes * 60\n",
    "        freq_buckets = defaultdict(int)\n",
    "        \n",
    "        for ts in timestamps:\n",
    "            bucket = ts // window_seconds\n",
    "            freq_buckets[bucket] += 1\n",
    "        \n",
    "        if len(freq_buckets) < 2:\n",
    "            return {\"frequency_anomaly\": False}\n",
    "        \n",
    "        frequencies = list(freq_buckets.values())\n",
    "        avg_freq = statistics.mean(frequencies)\n",
    "        max_freq = max(frequencies)\n",
    "        \n",
    "        # Anomaly if max frequency is 5x above average\n",
    "        is_anomaly = max_freq > (avg_freq * 5) and max_freq > 20\n",
    "        \n",
    "        return {\n",
    "            \"frequency_anomaly\": is_anomaly,\n",
    "            \"max_frequency\": max_freq,\n",
    "            \"avg_frequency\": avg_freq,\n",
    "            \"frequency_ratio\": max_freq / avg_freq if avg_freq > 0 else 0\n",
    "        }\n",
    "\n",
    "    def analyze_gas_patterns(self, txs: List[Dict]) -> Dict:\n",
    "        \"\"\"Detect gas price manipulation and MEV activity\"\"\"\n",
    "        if not txs:\n",
    "            return {\"gas_anomaly\": False}\n",
    "        \n",
    "        gas_prices = [int(tx['gasPrice']) for tx in txs if tx.get('gasPrice', '0') != '0']\n",
    "        gas_used = [int(tx['gasUsed']) for tx in txs if tx.get('gasUsed', '0') != '0']\n",
    "        failed_txs = [tx for tx in txs if tx.get('isError') == '1']\n",
    "        \n",
    "        if not gas_prices:\n",
    "            return {\"gas_anomaly\": False}\n",
    "        \n",
    "        avg_gas_price = statistics.mean(gas_prices)\n",
    "        max_gas_price = max(gas_prices)\n",
    "        min_gas_price = min(gas_prices)\n",
    "        \n",
    "        # Detect MEV/sandwich attacks (extremely high gas prices)\n",
    "        high_gas_count = len([g for g in gas_prices if g > avg_gas_price * 3])\n",
    "        failed_ratio = len(failed_txs) / len(txs) if txs else 0\n",
    "        \n",
    "        gas_variance = (max_gas_price / min_gas_price) if min_gas_price > 0 else 0\n",
    "        \n",
    "        is_anomaly = (\n",
    "            high_gas_count > 5 or  # Multiple high-gas transactions\n",
    "            failed_ratio > 0.3 or  # High failure rate\n",
    "            gas_variance > 100      # Extreme gas price variance\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"gas_anomaly\": is_anomaly,\n",
    "            \"high_gas_count\": high_gas_count,\n",
    "            \"failed_ratio\": failed_ratio,\n",
    "            \"gas_variance\": gas_variance,\n",
    "            \"avg_gas_price\": avg_gas_price\n",
    "        }\n",
    "\n",
    "    def analyze_value_patterns(self, txs: List[Dict]) -> Dict:\n",
    "        \"\"\"Detect suspicious value transfer patterns\"\"\"\n",
    "        if not txs:\n",
    "            return {\"value_anomaly\": False}\n",
    "        \n",
    "        values = [int(tx['value']) for tx in txs if int(tx.get('value', '0')) > 0]\n",
    "        \n",
    "        if not values:\n",
    "            return {\"value_anomaly\": False}\n",
    "        \n",
    "        # Detect round number bias (potential money laundering)\n",
    "        round_values = [v for v in values if v % (10**18) == 0]  # Exact ETH amounts\n",
    "        round_ratio = len(round_values) / len(values)\n",
    "        \n",
    "        # Detect dust attacks\n",
    "        dust_threshold = 10**15  # 0.001 ETH\n",
    "        dust_count = len([v for v in values if v < dust_threshold])\n",
    "        \n",
    "        # Detect value concentration\n",
    "        total_value = sum(values)\n",
    "        max_value = max(values)\n",
    "        concentration = max_value / total_value if total_value > 0 else 0\n",
    "        \n",
    "        is_anomaly = (\n",
    "            round_ratio > 0.7 or      # Too many round numbers\n",
    "            dust_count > 50 or        # Dust attack\n",
    "            concentration > 0.9       # Single large transaction dominates\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"value_anomaly\": is_anomaly,\n",
    "            \"round_number_ratio\": round_ratio,\n",
    "            \"dust_attack_count\": dust_count,\n",
    "            \"value_concentration\": concentration\n",
    "        }\n",
    "\n",
    "    def detect_bot_patterns(self, txs: List[Dict]) -> Dict:\n",
    "        \"\"\"Detect automated/bot activity patterns\"\"\"\n",
    "        if len(txs) < 5:\n",
    "            return {\"bot_detected\": False}\n",
    "        \n",
    "        timestamps = [int(tx['timeStamp']) for tx in txs]\n",
    "        timestamps.sort()\n",
    "        \n",
    "        # Calculate intervals between transactions\n",
    "        intervals = [timestamps[i+1] - timestamps[i] for i in range(len(timestamps)-1)]\n",
    "        \n",
    "        if not intervals:\n",
    "            return {\"bot_detected\": False}\n",
    "        \n",
    "        # Look for regular patterns (±3 seconds)\n",
    "        regular_intervals = []\n",
    "        for interval in intervals:\n",
    "            for target in [10, 15, 30, 60]:  # Common bot intervals\n",
    "                if abs(interval - target) <= 3:\n",
    "                    regular_intervals.append(interval)\n",
    "                    break\n",
    "        \n",
    "        regularity_ratio = len(regular_intervals) / len(intervals)\n",
    "        \n",
    "        # Check for identical gas prices (bot signature)\n",
    "        gas_prices = [tx.get('gasPrice', '0') for tx in txs]\n",
    "        unique_gas_prices = len(set(gas_prices))\n",
    "        gas_uniformity = 1 - (unique_gas_prices / len(gas_prices)) if gas_prices else 0\n",
    "        \n",
    "        is_bot = (\n",
    "            regularity_ratio > 0.8 or     # Very regular timing\n",
    "            gas_uniformity > 0.9          # Very uniform gas prices\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"bot_detected\": is_bot,\n",
    "            \"regularity_ratio\": regularity_ratio,\n",
    "            \"gas_uniformity\": gas_uniformity,\n",
    "            \"regular_interval_count\": len(regular_intervals)\n",
    "        }\n",
    "\n",
    "    async def webacy_sanction_check(self, address: str) -> Dict:\n",
    "        \"\"\"Quick sanction screening via Webacy\"\"\"\n",
    "        await self.init_session()\n",
    "        url = f\"https://api.webacy.com/addresses/sanctioned/{address}\"\n",
    "        headers = {\"accept\": \"application/json\", \"x-api-key\": WEBACY_API_KEY}\n",
    "        \n",
    "        try:\n",
    "            async with self.session.get(url, headers=headers) as response:\n",
    "                data = await response.json()\n",
    "                return {\n",
    "                    \"is_sanctioned\": data.get(\"is_sanctioned\", False),\n",
    "                    \"webacy_check\": True\n",
    "                }\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Webacy sanction check failed for {address}: {e}\")\n",
    "            return {\"is_sanctioned\": False, \"webacy_check\": False}\n",
    "\n",
    "    async def webacy_threat_analysis(self, address: str) -> Dict:\n",
    "        \"\"\"Full threat analysis via Webacy if needed\"\"\"\n",
    "        await self.init_session()\n",
    "        url = f\"https://api.webacy.com/addresses/{address}\"\n",
    "        headers = {\"accept\": \"application/json\", \"x-api-key\": WEBACY_API_KEY}\n",
    "        \n",
    "        try:\n",
    "            async with self.session.get(url, headers=headers) as response:\n",
    "                data = await response.json()\n",
    "                return {\n",
    "                    \"threat_score\": data.get(\"overallRisk\", 0),\n",
    "                    \"risk_level\": data.get(\"riskScore\", \"Unknown\"),\n",
    "                    \"issue_count\": data.get(\"count\", 0)\n",
    "                }\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Webacy threat analysis failed for {address}: {e}\")\n",
    "            return {\"threat_score\": 0, \"risk_level\": \"Unknown\", \"issue_count\": 0}\n",
    "\n",
    "    def calculate_combined_anomaly_score(self, patterns: Dict) -> tuple:\n",
    "        \"\"\"Calculate overall anomaly score and flags\"\"\"\n",
    "        score = 0\n",
    "        flags = []\n",
    "        \n",
    "        # Frequency anomalies\n",
    "        if patterns.get(\"frequency_anomaly\", False):\n",
    "            score += 35\n",
    "            flags.append(f\"High Frequency Activity ({patterns.get('max_frequency', 0)} txs)\")\n",
    "        \n",
    "        # Gas pattern anomalies\n",
    "        if patterns.get(\"gas_anomaly\", False):\n",
    "            score += 25\n",
    "            flags.append(\"Suspicious Gas Patterns\")\n",
    "            if patterns.get(\"failed_ratio\", 0) > 0.3:\n",
    "                flags.append(\"High Failed Transaction Rate\")\n",
    "        \n",
    "        # Value pattern anomalies\n",
    "        if patterns.get(\"value_anomaly\", False):\n",
    "            score += 20\n",
    "            if patterns.get(\"round_number_ratio\", 0) > 0.7:\n",
    "                flags.append(\"Round Number Bias\")\n",
    "            if patterns.get(\"dust_attack_count\", 0) > 50:\n",
    "                flags.append(\"Dust Attack Pattern\")\n",
    "        \n",
    "        # Bot detection\n",
    "        if patterns.get(\"bot_detected\", False):\n",
    "            score += 30\n",
    "            flags.append(\"Automated Activity Detected\")\n",
    "        \n",
    "        # Webacy threat intelligence\n",
    "        if patterns.get(\"is_sanctioned\", False):\n",
    "            score += 50\n",
    "            flags.append(\"SANCTIONED ADDRESS\")\n",
    "        \n",
    "        threat_score = patterns.get(\"threat_score\", 0)\n",
    "        if threat_score > 50:\n",
    "            score += 40\n",
    "            flags.append(f\"High Threat Score ({threat_score})\")\n",
    "        \n",
    "        # Determine risk level\n",
    "        if score >= 80:\n",
    "            risk_level = \"CRITICAL\"\n",
    "        elif score >= 60:\n",
    "            risk_level = \"HIGH\"\n",
    "        elif score >= 40:\n",
    "            risk_level = \"MEDIUM\"\n",
    "        else:\n",
    "            risk_level = \"LOW\"\n",
    "        \n",
    "        return min(score, 100), risk_level, flags\n",
    "\n",
    "    async def analyze_address(self, address: str) -> AddressAnalysis:\n",
    "        \"\"\"Comprehensive address analysis combining all detection methods\"\"\"\n",
    "        try:\n",
    "            # Quick sanction check first\n",
    "            sanction_result = await self.webacy_sanction_check(address)\n",
    "            \n",
    "            # Determine address type\n",
    "            is_contract_addr = await self.is_contract(address)\n",
    "            addr_type = \"Contract\" if is_contract_addr else \"EOA\"\n",
    "            \n",
    "            # Fetch recent transaction data\n",
    "            normal_txs = await self.fetch_transactions_batch(address, \"txlist\", 2)\n",
    "            internal_txs = await self.fetch_transactions_batch(address, \"txlistinternal\", 1)\n",
    "            token_txs = await self.fetch_transactions_batch(address, \"tokentx\", 1)\n",
    "            \n",
    "            # Analyze patterns\n",
    "            freq_analysis = self.analyze_transaction_frequency(normal_txs)\n",
    "            gas_analysis = self.analyze_gas_patterns(normal_txs)\n",
    "            value_analysis = self.analyze_value_patterns(normal_txs)\n",
    "            bot_analysis = self.detect_bot_patterns(normal_txs)\n",
    "            \n",
    "            # Combine all patterns\n",
    "            all_patterns = {\n",
    "                **freq_analysis,\n",
    "                **gas_analysis,\n",
    "                **value_analysis,\n",
    "                **bot_analysis,\n",
    "                **sanction_result\n",
    "            }\n",
    "            \n",
    "            # Get detailed threat analysis if anomalies detected\n",
    "            if any([freq_analysis.get(\"frequency_anomaly\"), gas_analysis.get(\"gas_anomaly\"), \n",
    "                   value_analysis.get(\"value_anomaly\"), bot_analysis.get(\"bot_detected\"),\n",
    "                   sanction_result.get(\"is_sanctioned\")]):\n",
    "                threat_data = await self.webacy_threat_analysis(address)\n",
    "                all_patterns.update(threat_data)\n",
    "            \n",
    "            # Calculate final score\n",
    "            score, risk_level, flags = self.calculate_combined_anomaly_score(all_patterns)\n",
    "            \n",
    "            return AddressAnalysis(\n",
    "                address=address,\n",
    "                address_type=addr_type,\n",
    "                anomaly_score=score,\n",
    "                risk_level=risk_level,\n",
    "                flags=flags,\n",
    "                webacy_sanctioned=sanction_result.get(\"is_sanctioned\", False),\n",
    "                webacy_threat_score=all_patterns.get(\"threat_score\")\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Analysis failed for {address}: {e}\")\n",
    "            raise HTTPException(status_code=500, detail=f\"Analysis failed: {str(e)}\")\n",
    "\n",
    "# Initialize detector\n",
    "detector = AnomalyDetector()\n",
    "\n",
    "@app.on_event(\"startup\")\n",
    "async def startup_event():\n",
    "    await detector.init_session()\n",
    "\n",
    "@app.on_event(\"shutdown\")\n",
    "async def shutdown_event():\n",
    "    await detector.close_session()\n",
    "\n",
    "@app.post(\"/analyze\", response_model=AddressAnalysis)\n",
    "async def analyze_address(address: str):\n",
    "    \"\"\"Analyze a single address for anomalies\"\"\"\n",
    "    return await detector.analyze_address(address.lower())\n",
    "\n",
    "@app.post(\"/monitor\")\n",
    "async def add_to_monitoring(address: str, background_tasks: BackgroundTasks):\n",
    "    \"\"\"Add address to continuous monitoring\"\"\"\n",
    "    address = address.lower()\n",
    "    detector.monitored_addresses.add(address)\n",
    "    background_tasks.add_task(continuous_monitoring, address)\n",
    "    return {\"message\": f\"Address {address} added to monitoring\"}\n",
    "\n",
    "@app.get(\"/monitored\")\n",
    "async def get_monitored_addresses():\n",
    "    \"\"\"Get list of currently monitored addresses\"\"\"\n",
    "    return {\"monitored_addresses\": list(detector.monitored_addresses)}\n",
    "\n",
    "async def continuous_monitoring(address: str):\n",
    "    \"\"\"Background task for continuous address monitoring\"\"\"\n",
    "    while address in detector.monitored_addresses:\n",
    "        try:\n",
    "            analysis = await detector.analyze_address(address)\n",
    "            \n",
    "            # Store anomaly if score is significant\n",
    "            if analysis.anomaly_score > 40:\n",
    "                anomaly_data = {\n",
    "                    \"timestamp\": datetime.now().isoformat(),\n",
    "                    \"address\": address,\n",
    "                    \"analysis\": analysis.dict()\n",
    "                }\n",
    "                anomaly_cache.append(anomaly_data)\n",
    "                logging.warning(f\"ANOMALY DETECTED: {address} - Score: {analysis.anomaly_score}\")\n",
    "            \n",
    "            # Monitor every 5 minutes\n",
    "            await asyncio.sleep(300)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Monitoring error for {address}: {e}\")\n",
    "            await asyncio.sleep(60)  # Retry after 1 minute on error\n",
    "\n",
    "@app.get(\"/anomalies\")\n",
    "async def get_recent_anomalies():\n",
    "    \"\"\"Get recent detected anomalies\"\"\"\n",
    "    return {\"recent_anomalies\": list(anomaly_cache)}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f145a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, BackgroundTasks, HTTPException\n",
    "from fastapi.responses import StreamingResponse\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import time\n",
    "import json\n",
    "from typing import Dict, List, Optional, Set\n",
    "from datetime import datetime, timedelta\n",
    "from pydantic import BaseModel\n",
    "import logging\n",
    "from collections import defaultdict\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "app = FastAPI(title=\"Blockchain Anomaly Detection System\", version=\"1.0.0\")\n",
    "\n",
    "# Pydantic models for API responses\n",
    "class AnomalyResult(BaseModel):\n",
    "    address: str\n",
    "    anomaly_score: int\n",
    "    risk_level: str\n",
    "    flags: List[str]\n",
    "    analysis_timestamp: str\n",
    "    \n",
    "class TransactionSummary(BaseModel):\n",
    "    total_transactions: int\n",
    "    unique_addresses: int\n",
    "    anomaly_count: int\n",
    "    high_risk_addresses: List[str]\n",
    "\n",
    "class BlockchainAnomalyDetector:\n",
    "    def __init__(self, api_key: str):\n",
    "        self.api_key = api_key\n",
    "        self.contract_cache: Dict[str, bool] = {}\n",
    "        self.anomaly_results: Dict[str, Dict] = {}\n",
    "        self.base_url = \"https://api.etherscan.io\"\n",
    "        self.rate_limit_delay = 0.2  # 200ms between requests\n",
    "        \n",
    "    async def get_transactions_async(self, address: str, action: str = \"txlist\", \n",
    "                                   chainid: int = 1, offset: int = 1000, \n",
    "                                   max_pages: int = 5) -> List[Dict]:\n",
    "        \"\"\"Async version of transaction fetching\"\"\"\n",
    "        transactions = []\n",
    "        \n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            for page in range(1, max_pages + 1):\n",
    "                url = (\n",
    "                    f\"{self.base_url}/v2/api\"\n",
    "                    f\"?chainid={chainid}\"\n",
    "                    f\"&module=account\"\n",
    "                    f\"&action={action}\"\n",
    "                    f\"&address={address}\"\n",
    "                    f\"&startblock=0\"\n",
    "                    f\"&endblock=latest\"\n",
    "                    f\"&page={page}\"\n",
    "                    f\"&offset={offset}\"\n",
    "                    f\"&sort=desc\"\n",
    "                    f\"&apikey={self.api_key}\"\n",
    "                )\n",
    "                \n",
    "                try:\n",
    "                    async with session.get(url) as response:\n",
    "                        data = await response.json()\n",
    "                        txs = data.get(\"result\", [])\n",
    "                        \n",
    "                        if not txs or txs == \"No transactions found\":\n",
    "                            break\n",
    "                            \n",
    "                        transactions.extend(txs)\n",
    "                        logger.info(f\"Fetched page {page}, got {len(txs)} {action} transactions\")\n",
    "                        \n",
    "                        # Rate limiting\n",
    "                        await asyncio.sleep(self.rate_limit_delay)\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error fetching {action} transactions page {page}: {e}\")\n",
    "                    break\n",
    "                    \n",
    "        return transactions\n",
    "\n",
    "    async def is_contract_async(self, address: str) -> bool:\n",
    "        \"\"\"Async version of contract checking with caching\"\"\"\n",
    "        if address in self.contract_cache:\n",
    "            return self.contract_cache[address]\n",
    "        \n",
    "        url = f\"{self.base_url}/api?module=proxy&action=eth_getCode&address={address}&tag=latest&apikey={self.api_key}\"\n",
    "        \n",
    "        try:\n",
    "            async with aiohttp.ClientSession() as session:\n",
    "                async with session.get(url) as response:\n",
    "                    data = await response.json()\n",
    "                    result = data.get(\"result\", \"0x\") != \"0x\"\n",
    "                    self.contract_cache[address] = result\n",
    "                    await asyncio.sleep(self.rate_limit_delay)\n",
    "                    return result\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error checking contract status for {address}: {e}\")\n",
    "            return False\n",
    "\n",
    "    async def enrich_transactions(self, transactions: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Add contract/EOA labels and other enrichments\"\"\"\n",
    "        enriched = []\n",
    "        \n",
    "        for tx in transactions:\n",
    "            from_addr = tx.get(\"from\", \"\")\n",
    "            to_addr = tx.get(\"to\", \"\")\n",
    "            \n",
    "            # Add address types\n",
    "            if from_addr:\n",
    "                tx[\"from_type\"] = \"Contract\" if await self.is_contract_async(from_addr) else \"EOA\"\n",
    "            if to_addr:\n",
    "                tx[\"to_type\"] = \"Contract\" if await self.is_contract_async(to_addr) else \"EOA\"\n",
    "            \n",
    "            # Add scaled values for token transactions\n",
    "            if \"tokenDecimal\" in tx:\n",
    "                decimals = int(tx.get(\"tokenDecimal\", 0))\n",
    "                tx[\"value_scaled\"] = int(tx.get(\"value\", 0)) / (10 ** decimals) if decimals else int(tx.get(\"value\", 0))\n",
    "            \n",
    "            enriched.append(tx)\n",
    "            \n",
    "        return enriched\n",
    "\n",
    "    # Anomaly Detection Methods\n",
    "    def analyze_transaction_frequency(self, txs: List[Dict], window_minutes: int = 60) -> Dict:\n",
    "        \"\"\"Detect transaction frequency anomalies\"\"\"\n",
    "        freq_patterns = {}\n",
    "        for tx in txs:\n",
    "            timestamp = int(tx.get('timeStamp', 0))\n",
    "            window_key = timestamp // (window_minutes * 60)\n",
    "            freq_patterns[window_key] = freq_patterns.get(window_key, 0) + 1\n",
    "        \n",
    "        if not freq_patterns:\n",
    "            return {\"anomalies\": [], \"avg_frequency\": 0}\n",
    "            \n",
    "        avg_freq = sum(freq_patterns.values()) / len(freq_patterns)\n",
    "        anomalies = [k for k, v in freq_patterns.items() if v > avg_freq * 5]\n",
    "        \n",
    "        return {\n",
    "            \"anomalies\": anomalies,\n",
    "            \"avg_frequency\": avg_freq,\n",
    "            \"spike_count\": len(anomalies)\n",
    "        }\n",
    "\n",
    "    def detect_bot_activity(self, txs: List[Dict]) -> Dict:\n",
    "        \"\"\"Detect bot-like regular interval patterns\"\"\"\n",
    "        timestamps = [int(tx.get('timeStamp', 0)) for tx in txs]\n",
    "        timestamps.sort()\n",
    "        \n",
    "        if len(timestamps) < 3:\n",
    "            return {\"bot_suspected\": False, \"regular_interval_count\": 0}\n",
    "            \n",
    "        intervals = [timestamps[i+1] - timestamps[i] for i in range(len(timestamps)-1)]\n",
    "        \n",
    "        # Bot patterns: very regular intervals (±2 seconds)\n",
    "        regular_intervals = [i for i in intervals if 8 <= i <= 12]  # 10-second bot pattern\n",
    "        \n",
    "        return {\n",
    "            \"bot_suspected\": len(regular_intervals) > 10,\n",
    "            \"regular_interval_count\": len(regular_intervals),\n",
    "            \"total_intervals\": len(intervals),\n",
    "            \"regularity_ratio\": len(regular_intervals) / len(intervals) if intervals else 0\n",
    "        }\n",
    "\n",
    "    def analyze_value_anomalies(self, txs: List[Dict]) -> Dict:\n",
    "        \"\"\"Detect unusual value patterns\"\"\"\n",
    "        values = []\n",
    "        for tx in txs:\n",
    "            if 'value_scaled' in tx:\n",
    "                values.append(float(tx['value_scaled']))\n",
    "            elif 'value' in tx:\n",
    "                val = int(tx['value'])\n",
    "                if val > 0:\n",
    "                    values.append(val)\n",
    "        \n",
    "        if not values:\n",
    "            return {\"round_number_ratio\": 0, \"dust_attack_count\": 0, \"value_concentration\": 0}\n",
    "        \n",
    "        avg_value = sum(values) / len(values)\n",
    "        \n",
    "        # Detect round number bias (money laundering indicator)\n",
    "        round_values = [v for v in values if v % 1 == 0 and v > 0]  # Round numbers\n",
    "        \n",
    "        # Detect dust attacks\n",
    "        dust_threshold = 0.001 if 'value_scaled' in txs[0] else 10**15  # 0.001 ETH\n",
    "        dust_txs = [v for v in values if v < dust_threshold]\n",
    "        \n",
    "        return {\n",
    "            \"round_number_ratio\": len(round_values) / len(values) if values else 0,\n",
    "            \"dust_attack_count\": len(dust_txs),\n",
    "            \"value_concentration\": max(values) / avg_value if avg_value > 0 else 0,\n",
    "            \"value_variance\": (max(values) - min(values)) / avg_value if avg_value > 0 else 0\n",
    "        }\n",
    "\n",
    "    def analyze_gas_patterns(self, txs: List[Dict]) -> Dict:\n",
    "        \"\"\"Analyze gas usage patterns for anomalies\"\"\"\n",
    "        gas_prices = [int(tx.get('gasPrice', 0)) for tx in txs if tx.get('gasPrice')]\n",
    "        gas_used = [int(tx.get('gasUsed', 0)) for tx in txs if tx.get('gasUsed')]\n",
    "        \n",
    "        if not gas_prices:\n",
    "            return {\"high_gas_count\": 0, \"failed_tx_ratio\": 0, \"gas_price_variance\": 0}\n",
    "            \n",
    "        avg_gas_price = sum(gas_prices) / len(gas_prices)\n",
    "        \n",
    "        # Detect MEV/sandwich attacks: unusually high gas prices\n",
    "        high_gas_txs = [g for g in gas_prices if g > avg_gas_price * 3]\n",
    "        \n",
    "        # Detect failed transaction spams\n",
    "        failed_txs = [tx for tx in txs if tx.get('isError') == '1']\n",
    "        \n",
    "        return {\n",
    "            \"high_gas_count\": len(high_gas_txs),\n",
    "            \"failed_tx_ratio\": len(failed_txs) / len(txs) if txs else 0,\n",
    "            \"gas_price_variance\": max(gas_prices) / min(gas_prices) if gas_prices and min(gas_prices) > 0 else 0,\n",
    "            \"avg_gas_price\": avg_gas_price\n",
    "        }\n",
    "\n",
    "    def analyze_address_relationships(self, normal_txs: List[Dict], internal_txs: List[Dict], token_txs: List[Dict]) -> Dict:\n",
    "        \"\"\"Analyze interaction patterns between addresses\"\"\"\n",
    "        all_addresses = set()\n",
    "        interactions = {}\n",
    "        \n",
    "        # Build interaction graph\n",
    "        for tx_list, tx_type in [(normal_txs, 'normal'), (internal_txs, 'internal'), (token_txs, 'token')]:\n",
    "            for tx in tx_list:\n",
    "                from_addr = tx.get('from', '')\n",
    "                to_addr = tx.get('to', '')\n",
    "                \n",
    "                if from_addr and to_addr:\n",
    "                    key = (from_addr, to_addr)\n",
    "                    if key not in interactions:\n",
    "                        interactions[key] = {'count': 0, 'types': set(), 'values': []}\n",
    "                    interactions[key]['count'] += 1\n",
    "                    interactions[key]['types'].add(tx_type)\n",
    "                    \n",
    "                    # Add value if available\n",
    "                    value = tx.get('value_scaled', tx.get('value', 0))\n",
    "                    if value:\n",
    "                        interactions[key]['values'].append(float(value))\n",
    "                    \n",
    "                    all_addresses.update([from_addr, to_addr])\n",
    "        \n",
    "        # Detect suspicious patterns\n",
    "        circular_flows = self.detect_circular_transactions(interactions)\n",
    "        high_interaction_pairs = [(k, v['count']) for k, v in interactions.items() if v['count'] > 50]\n",
    "        \n",
    "        return {\n",
    "            \"unique_addresses\": len(all_addresses),\n",
    "            \"interaction_pairs\": len(interactions),\n",
    "            \"circular_flows\": len(circular_flows),\n",
    "            \"high_interaction_pairs\": len(high_interaction_pairs),\n",
    "            \"avg_interactions_per_pair\": sum(v['count'] for v in interactions.values()) / len(interactions) if interactions else 0\n",
    "        }\n",
    "\n",
    "    def detect_circular_transactions(self, interactions: Dict) -> List[Dict]:\n",
    "        \"\"\"Detect circular transaction patterns\"\"\"\n",
    "        circular = []\n",
    "        interaction_graph = {}\n",
    "        \n",
    "        # Build adjacency list\n",
    "        for (from_addr, to_addr), data in interactions.items():\n",
    "            if from_addr not in interaction_graph:\n",
    "                interaction_graph[from_addr] = []\n",
    "            interaction_graph[from_addr].append((to_addr, data))\n",
    "        \n",
    "        # Look for simple circular patterns (A->B->A)\n",
    "        for addr_a in interaction_graph:\n",
    "            for addr_b, data_ab in interaction_graph[addr_a]:\n",
    "                if addr_b in interaction_graph:\n",
    "                    for addr_c, data_bc in interaction_graph[addr_b]:\n",
    "                        if addr_c == addr_a:  # Found circle\n",
    "                            circular.append({\n",
    "                                \"addresses\": [addr_a, addr_b],\n",
    "                                \"ab_count\": data_ab['count'],\n",
    "                                \"ba_count\": data_bc['count']\n",
    "                            })\n",
    "        \n",
    "        return circular\n",
    "\n",
    "    def detect_coordinated_patterns(self, normal_txs: List[Dict], internal_txs: List[Dict], token_txs: List[Dict]) -> Dict:\n",
    "        \"\"\"Detect coordinated activity across transaction types\"\"\"\n",
    "        timestamps = {\n",
    "            'normal': set(tx.get('timeStamp', '') for tx in normal_txs),\n",
    "            'internal': set(tx.get('timeStamp', '') for tx in internal_txs),\n",
    "            'token': set(tx.get('timeStamp', '') for tx in token_txs)\n",
    "        }\n",
    "        \n",
    "        # Remove empty timestamps\n",
    "        for key in timestamps:\n",
    "            timestamps[key].discard('')\n",
    "        \n",
    "        all_times = timestamps['normal'] | timestamps['internal'] | timestamps['token']\n",
    "        coordinated_times = []\n",
    "        \n",
    "        for time in all_times:\n",
    "            types_at_time = []\n",
    "            if time in timestamps['normal']: types_at_time.append('normal')\n",
    "            if time in timestamps['internal']: types_at_time.append('internal')  \n",
    "            if time in timestamps['token']: types_at_time.append('token')\n",
    "            \n",
    "            if len(types_at_time) >= 2:\n",
    "                coordinated_times.append({\n",
    "                    \"timestamp\": time,\n",
    "                    \"types\": types_at_time\n",
    "                })\n",
    "        \n",
    "        return {\n",
    "            \"coordinated_timestamp_count\": len(coordinated_times),\n",
    "            \"coordination_ratio\": len(coordinated_times) / len(all_times) if all_times else 0,\n",
    "            \"coordinated_events\": coordinated_times[:10]  # Sample of events\n",
    "        }\n",
    "\n",
    "    def calculate_anomaly_score(self, address: str, analysis_data: Dict) -> Dict:\n",
    "        \"\"\"Calculate overall anomaly score\"\"\"\n",
    "        score = 0\n",
    "        flags = []\n",
    "        \n",
    "        # Bot activity detection\n",
    "        if analysis_data.get('bot_activity', {}).get('bot_suspected', False):\n",
    "            score += 30\n",
    "            flags.append(\"Bot Activity Detected\")\n",
    "        \n",
    "        # Value pattern anomalies\n",
    "        round_ratio = analysis_data.get('value_patterns', {}).get('round_number_ratio', 0)\n",
    "        if round_ratio > 0.5:\n",
    "            score += 20\n",
    "            flags.append(\"Round Number Bias\")\n",
    "        \n",
    "        # High dust attack count\n",
    "        dust_count = analysis_data.get('value_patterns', {}).get('dust_attack_count', 0)\n",
    "        if dust_count > 100:\n",
    "            score += 15\n",
    "            flags.append(\"Potential Dust Attack\")\n",
    "        \n",
    "        # Failed transaction spam\n",
    "        failed_ratio = analysis_data.get('gas_patterns', {}).get('failed_tx_ratio', 0)\n",
    "        if failed_ratio > 0.2:\n",
    "            score += 25\n",
    "            flags.append(\"High Failed Transaction Rate\")\n",
    "        \n",
    "        # Coordinated activity\n",
    "        coord_ratio = analysis_data.get('coordination', {}).get('coordination_ratio', 0)\n",
    "        if coord_ratio > 0.3:\n",
    "            score += 40\n",
    "            flags.append(\"Coordinated Multi-Type Transactions\")\n",
    "        \n",
    "        # Frequency spikes\n",
    "        spike_count = analysis_data.get('frequency', {}).get('spike_count', 0)\n",
    "        if spike_count > 5:\n",
    "            score += 20\n",
    "            flags.append(\"Transaction Frequency Spikes\")\n",
    "        \n",
    "        # Circular transaction patterns\n",
    "        circular_count = analysis_data.get('relationships', {}).get('circular_flows', 0)\n",
    "        if circular_count > 0:\n",
    "            score += 35\n",
    "            flags.append(\"Circular Transaction Patterns\")\n",
    "        \n",
    "        # High gas price variance (MEV activity)\n",
    "        gas_variance = analysis_data.get('gas_patterns', {}).get('gas_price_variance', 0)\n",
    "        if gas_variance > 10:\n",
    "            score += 25\n",
    "            flags.append(\"Unusual Gas Price Patterns\")\n",
    "        \n",
    "        return {\n",
    "            \"address\": address,\n",
    "            \"anomaly_score\": min(score, 100),\n",
    "            \"risk_level\": \"HIGH\" if score > 70 else \"MEDIUM\" if score > 40 else \"LOW\",\n",
    "            \"flags\": flags,\n",
    "            \"analysis_timestamp\": datetime.now().isoformat()\n",
    "        }\n",
    "\n",
    "    async def analyze_address_comprehensive(self, address: str) -> Dict:\n",
    "        \"\"\"Comprehensive analysis of an address\"\"\"\n",
    "        logger.info(f\"Starting comprehensive analysis for address: {address}\")\n",
    "        \n",
    "        # Fetch all transaction types\n",
    "        normal_txs = await self.get_transactions_async(address, \"txlist\")\n",
    "        internal_txs = await self.get_transactions_async(address, \"txlistinternal\")  \n",
    "        token_txs = await self.get_transactions_async(address, \"tokentx\")\n",
    "        \n",
    "        # Enrich transactions\n",
    "        normal_txs = await self.enrich_transactions(normal_txs)\n",
    "        internal_txs = await self.enrich_transactions(internal_txs)\n",
    "        token_txs = await self.enrich_transactions(token_txs)\n",
    "        \n",
    "        # Perform all analyses\n",
    "        analysis_data = {\n",
    "            \"frequency\": self.analyze_transaction_frequency(normal_txs + internal_txs + token_txs),\n",
    "            \"bot_activity\": self.detect_bot_activity(normal_txs + internal_txs + token_txs),\n",
    "            \"value_patterns\": self.analyze_value_anomalies(token_txs + [tx for tx in normal_txs if int(tx.get('value', 0)) > 0]),\n",
    "            \"gas_patterns\": self.analyze_gas_patterns(normal_txs),\n",
    "            \"relationships\": self.analyze_address_relationships(normal_txs, internal_txs, token_txs),\n",
    "            \"coordination\": self.detect_coordinated_patterns(normal_txs, internal_txs, token_txs)\n",
    "        }\n",
    "        \n",
    "        # Calculate anomaly score\n",
    "        anomaly_result = self.calculate_anomaly_score(address, analysis_data)\n",
    "        \n",
    "        # Store results\n",
    "        self.anomaly_results[address] = {\n",
    "            \"anomaly_result\": anomaly_result,\n",
    "            \"detailed_analysis\": analysis_data,\n",
    "            \"transaction_counts\": {\n",
    "                \"normal\": len(normal_txs),\n",
    "                \"internal\": len(internal_txs),\n",
    "                \"token\": len(token_txs)\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        logger.info(f\"Analysis complete for {address}. Risk level: {anomaly_result['risk_level']}\")\n",
    "        return self.anomaly_results[address]\n",
    "\n",
    "# Global detector instance\n",
    "detector = None\n",
    "\n",
    "@app.on_event(\"startup\")\n",
    "async def startup_event():\n",
    "    global detector\n",
    "    # Replace with your actual API key\n",
    "    api_key = \"YOUR_ETHERSCAN_API_KEY\"\n",
    "    detector = BlockchainAnomalyDetector(api_key)\n",
    "    logger.info(\"Blockchain Anomaly Detection System started\")\n",
    "\n",
    "@app.get(\"/\", summary=\"Health Check\")\n",
    "async def root():\n",
    "    return {\"message\": \"Blockchain Anomaly Detection System is running\"}\n",
    "\n",
    "@app.post(\"/analyze/{address}\", response_model=AnomalyResult, summary=\"Analyze Address for Anomalies\")\n",
    "async def analyze_address(address: str):\n",
    "    \"\"\"Perform comprehensive anomaly analysis on a blockchain address\"\"\"\n",
    "    if not detector:\n",
    "        raise HTTPException(status_code=500, detail=\"Detector not initialized\")\n",
    "    \n",
    "    try:\n",
    "        result = await detector.analyze_address_comprehensive(address)\n",
    "        return AnomalyResult(**result[\"anomaly_result\"])\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error analyzing address {address}: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=f\"Analysis failed: {str(e)}\")\n",
    "\n",
    "@app.get(\"/results/{address}\", summary=\"Get Analysis Results\")\n",
    "async def get_analysis_results(address: str):\n",
    "    \"\"\"Get detailed analysis results for an address\"\"\"\n",
    "    if not detector or address not in detector.anomaly_results:\n",
    "        raise HTTPException(status_code=404, detail=\"Analysis results not found\")\n",
    "    \n",
    "    return detector.anomaly_results[address]\n",
    "\n",
    "@app.get(\"/summary\", response_model=TransactionSummary, summary=\"Get Analysis Summary\")\n",
    "async def get_summary():\n",
    "    \"\"\"Get summary of all analyses performed\"\"\"\n",
    "    if not detector:\n",
    "        raise HTTPException(status_code=500, detail=\"Detector not initialized\")\n",
    "    \n",
    "    results = detector.anomaly_results\n",
    "    high_risk = [addr for addr, data in results.items() \n",
    "                 if data[\"anomaly_result\"][\"risk_level\"] == \"HIGH\"]\n",
    "    \n",
    "    return TransactionSummary(\n",
    "        total_transactions=sum(\n",
    "            sum(data[\"transaction_counts\"].values()) \n",
    "            for data in results.values()\n",
    "        ),\n",
    "        unique_addresses=len(results),\n",
    "        anomaly_count=len([r for r in results.values() if r[\"anomaly_result\"][\"anomaly_score\"] > 40]),\n",
    "        high_risk_addresses=high_risk\n",
    "    )\n",
    "\n",
    "@app.get(\"/batch-analyze\", summary=\"Batch Analyze Multiple Addresses\")\n",
    "async def batch_analyze(addresses: str):\n",
    "    \"\"\"Analyze multiple addresses separated by commas\"\"\"\n",
    "    if not detector:\n",
    "        raise HTTPException(status_code=500, detail=\"Detector not initialized\")\n",
    "    \n",
    "    addr_list = [addr.strip() for addr in addresses.split(\",\") if addr.strip()]\n",
    "    \n",
    "    if len(addr_list) > 10:\n",
    "        raise HTTPException(status_code=400, detail=\"Maximum 10 addresses allowed per batch\")\n",
    "    \n",
    "    results = []\n",
    "    for address in addr_list:\n",
    "        try:\n",
    "            result = await detector.analyze_address_comprehensive(address)\n",
    "            results.append(result[\"anomaly_result\"])\n",
    "        except Exception as e:\n",
    "            results.append({\n",
    "                \"address\": address,\n",
    "                \"error\": str(e),\n",
    "                \"analysis_timestamp\": datetime.now().isoformat()\n",
    "            })\n",
    "    \n",
    "    return {\"batch_results\": results}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38857775",
   "metadata": {},
   "outputs": [],
   "source": [
    "doesn't follow real time\n",
    "\n",
    "import requests\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Set, Tuple\n",
    "import json\n",
    "\n",
    "class EtherscanAnomalyDetector:\n",
    "    def __init__(self, api_key: str):\n",
    "        self.api_key = api_key\n",
    "        self.contract_cache = {}\n",
    "        self.base_url = \"https://api.etherscan.io\"\n",
    "        \n",
    "    # ======================== DATA COLLECTION ========================\n",
    "    \n",
    "    def get_transactions(self, address: str, action=\"txlist\", chainid=1, offset=1000, max_pages=5):\n",
    "        \"\"\"Fetch transactions with pagination\"\"\"\n",
    "        transactions = []\n",
    "        for page in range(1, max_pages + 1):\n",
    "            url = (\n",
    "                f\"{self.base_url}/v2/api\"\n",
    "                f\"?chainid={chainid}\"\n",
    "                f\"&module=account\"\n",
    "                f\"&action={action}\"\n",
    "                f\"&address={address}\"\n",
    "                f\"&startblock=0\"\n",
    "                f\"&endblock=latest\"\n",
    "                f\"&page={page}\"\n",
    "                f\"&offset={offset}\"\n",
    "                f\"&sort=desc\"\n",
    "                f\"&apikey={self.api_key}\"\n",
    "            )\n",
    "            \n",
    "            response = requests.get(url).json()\n",
    "            txs = response.get(\"result\", [])\n",
    "            \n",
    "            if not txs or txs == \"No transactions found\":\n",
    "                break\n",
    "                \n",
    "            transactions.extend(txs)\n",
    "            print(f\"Fetched page {page}, got {len(txs)} {action} transactions\")\n",
    "            time.sleep(0.2)  # Rate limiting\n",
    "            \n",
    "        return transactions\n",
    "    \n",
    "    def is_contract(self, address: str) -> bool:\n",
    "        \"\"\"Check if address is contract with caching\"\"\"\n",
    "        if address in self.contract_cache:\n",
    "            return self.contract_cache[address]\n",
    "        \n",
    "        url = f\"{self.base_url}/api?module=proxy&action=eth_getCode&address={address}&tag=latest&apikey={self.api_key}\"\n",
    "        response = requests.get(url).json()\n",
    "        result = response.get(\"result\", \"0x\") != \"0x\"\n",
    "        \n",
    "        self.contract_cache[address] = result\n",
    "        return result\n",
    "    \n",
    "    def enrich_normal_transactions(self, txs: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Enrich normal transactions with address types\"\"\"\n",
    "        enriched = []\n",
    "        for tx in txs:\n",
    "            from_addr = tx[\"from\"]\n",
    "            to_addr = tx[\"to\"]\n",
    "            \n",
    "            tx[\"from_type\"] = \"Contract\" if self.is_contract(from_addr) else \"EOA\"\n",
    "            tx[\"to_type\"] = \"Contract\" if self.is_contract(to_addr) else \"EOA\"\n",
    "            \n",
    "            enriched.append(tx)\n",
    "        return enriched\n",
    "    \n",
    "    def enrich_internal_transactions(self, txs: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Enrich internal transactions with address types\"\"\"\n",
    "        enriched = []\n",
    "        for tx in txs:\n",
    "            from_addr = tx.get(\"from\")\n",
    "            to_addr = tx.get(\"to\")\n",
    "            \n",
    "            if from_addr:\n",
    "                tx[\"from_type\"] = \"Contract\" if self.is_contract(from_addr) else \"EOA\"\n",
    "            if to_addr:\n",
    "                tx[\"to_type\"] = \"Contract\" if self.is_contract(to_addr) else \"EOA\"\n",
    "            \n",
    "            enriched.append(tx)\n",
    "        return enriched\n",
    "    \n",
    "    def enrich_token_transactions(self, txs: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Enrich token transactions with address types and scaled values\"\"\"\n",
    "        # First, classify all unique addresses at once for efficiency\n",
    "        unique_addresses = set()\n",
    "        for tx in txs:\n",
    "            unique_addresses.add(tx[\"from\"])\n",
    "            unique_addresses.add(tx[\"to\"])\n",
    "        \n",
    "        labels = {}\n",
    "        for addr in unique_addresses:\n",
    "            labels[addr] = \"Contract\" if self.is_contract(addr) else \"EOA\"\n",
    "            time.sleep(0.1)  # Rate limiting\n",
    "        \n",
    "        # Now enrich transactions\n",
    "        enriched = []\n",
    "        for tx in txs:\n",
    "            tx[\"from_type\"] = labels.get(tx[\"from\"], \"Unknown\")\n",
    "            tx[\"to_type\"] = labels.get(tx[\"to\"], \"Unknown\")\n",
    "            \n",
    "            # Scale token values\n",
    "            decimals = int(tx.get(\"tokenDecimal\", 0))\n",
    "            tx[\"value_scaled\"] = int(tx[\"value\"]) / (10 ** decimals) if decimals else int(tx[\"value\"])\n",
    "            \n",
    "            enriched.append(tx)\n",
    "        \n",
    "        return enriched\n",
    "    \n",
    "    # ======================== ANOMALY DETECTION ========================\n",
    "    \n",
    "    def analyze_transaction_frequency(self, txs: List[Dict], window_minutes=60) -> Dict:\n",
    "        \"\"\"Detect transaction frequency anomalies\"\"\"\n",
    "        freq_patterns = {}\n",
    "        for tx in txs:\n",
    "            timestamp = int(tx.get('timeStamp', 0))\n",
    "            window_key = timestamp // (window_minutes * 60)\n",
    "            freq_patterns[window_key] = freq_patterns.get(window_key, 0) + 1\n",
    "        \n",
    "        if not freq_patterns:\n",
    "            return {\"anomalies\": [], \"avg_frequency\": 0, \"spike_count\": 0}\n",
    "        \n",
    "        avg_freq = sum(freq_patterns.values()) / len(freq_patterns)\n",
    "        anomalies = [k for k, v in freq_patterns.items() if v > avg_freq * 5]\n",
    "        \n",
    "        return {\n",
    "            \"anomalies\": anomalies,\n",
    "            \"avg_frequency\": avg_freq,\n",
    "            \"spike_count\": len(anomalies),\n",
    "            \"max_frequency\": max(freq_patterns.values()) if freq_patterns else 0\n",
    "        }\n",
    "    \n",
    "    def detect_bot_activity(self, txs: List[Dict]) -> Dict:\n",
    "        \"\"\"Detect bot-like regular interval patterns\"\"\"\n",
    "        timestamps = [int(tx.get('timeStamp', 0)) for tx in txs]\n",
    "        timestamps.sort()\n",
    "        \n",
    "        if len(timestamps) < 3:\n",
    "            return {\"bot_suspected\": False, \"regular_interval_count\": 0}\n",
    "        \n",
    "        intervals = [timestamps[i+1] - timestamps[i] for i in range(len(timestamps)-1)]\n",
    "        \n",
    "        # Bot patterns: very regular intervals (±2 seconds)\n",
    "        regular_intervals = [i for i in intervals if 8 <= i <= 12]  # 10-second bot pattern\n",
    "        \n",
    "        return {\n",
    "            \"bot_suspected\": len(regular_intervals) > 10,\n",
    "            \"regular_interval_count\": len(regular_intervals),\n",
    "            \"total_intervals\": len(intervals),\n",
    "            \"regularity_ratio\": len(regular_intervals) / len(intervals) if intervals else 0\n",
    "        }\n",
    "    \n",
    "    def analyze_value_anomalies(self, txs: List[Dict]) -> Dict:\n",
    "        \"\"\"Detect unusual value patterns - works with both normal and token transactions\"\"\"\n",
    "        values = []\n",
    "        \n",
    "        for tx in txs:\n",
    "            # For token transactions, use scaled values\n",
    "            if 'value_scaled' in tx and tx['value_scaled'] > 0:\n",
    "                values.append(float(tx['value_scaled']))\n",
    "            # For normal transactions, use raw values\n",
    "            elif 'value' in tx and int(tx['value']) > 0:\n",
    "                values.append(int(tx['value']))\n",
    "        \n",
    "        if not values:\n",
    "            return {\"round_number_ratio\": 0, \"dust_attack_count\": 0, \"value_concentration\": 0}\n",
    "        \n",
    "        avg_value = sum(values) / len(values)\n",
    "        \n",
    "        # Detect round number bias (money laundering indicator)\n",
    "        # For token txs: check if scaled value is round (1.0, 10.0, 100.0)\n",
    "        # For ETH txs: check if value is exact ETH amounts\n",
    "        round_values = []\n",
    "        dust_txs = []\n",
    "        \n",
    "        for tx, value in zip(txs, values):\n",
    "            if 'value_scaled' in tx:  # Token transaction\n",
    "                if value > 0 and (value == int(value) or value in [0.1, 1.0, 10.0, 100.0, 1000.0]):\n",
    "                    round_values.append(value)\n",
    "                if value < 0.001:  # Dust threshold for tokens\n",
    "                    dust_txs.append(value)\n",
    "            else:  # ETH transaction\n",
    "                if value % (10**18) == 0:  # Exact ETH amounts\n",
    "                    round_values.append(value)\n",
    "                if value < 10**15:  # < 0.001 ETH\n",
    "                    dust_txs.append(value)\n",
    "        \n",
    "        return {\n",
    "            \"round_number_ratio\": len(round_values) / len(values) if values else 0,\n",
    "            \"dust_attack_count\": len(dust_txs),\n",
    "            \"value_concentration\": max(values) / avg_value if avg_value > 0 else 0,\n",
    "            \"total_value_transfers\": len(values)\n",
    "        }\n",
    "    \n",
    "    def analyze_gas_patterns(self, txs: List[Dict]) -> Dict:\n",
    "        \"\"\"Analyze gas usage patterns - only for normal transactions\"\"\"\n",
    "        gas_prices = [int(tx.get('gasPrice', 0)) for tx in txs if tx.get('gasPrice')]\n",
    "        gas_used = [int(tx.get('gasUsed', 0)) for tx in txs if tx.get('gasUsed')]\n",
    "        \n",
    "        if not gas_prices:\n",
    "            return {\"high_gas_count\": 0, \"failed_tx_ratio\": 0, \"gas_price_variance\": 0}\n",
    "        \n",
    "        avg_gas_price = sum(gas_prices) / len(gas_prices)\n",
    "        \n",
    "        # Detect MEV/sandwich attacks: unusually high gas prices\n",
    "        high_gas_txs = [g for g in gas_prices if g > avg_gas_price * 3]\n",
    "        \n",
    "        # Detect failed transaction spams\n",
    "        failed_txs = [tx for tx in txs if tx.get('isError') == '1']\n",
    "        \n",
    "        return {\n",
    "            \"high_gas_count\": len(high_gas_txs),\n",
    "            \"failed_tx_ratio\": len(failed_txs) / len(txs) if txs else 0,\n",
    "            \"gas_price_variance\": max(gas_prices) / min(gas_prices) if gas_prices and min(gas_prices) > 0 else 0,\n",
    "            \"avg_gas_price\": avg_gas_price,\n",
    "            \"total_gas_used\": sum(gas_used)\n",
    "        }\n",
    "    \n",
    "    def analyze_address_relationships(self, normal_txs: List[Dict], internal_txs: List[Dict], token_txs: List[Dict]) -> Dict:\n",
    "        \"\"\"Analyze interaction patterns between addresses across all transaction types\"\"\"\n",
    "        all_addresses = set()\n",
    "        interactions = {}\n",
    "        \n",
    "        # Build interaction graph from all transaction types\n",
    "        for tx_list, tx_type in [(normal_txs, 'normal'), (internal_txs, 'internal'), (token_txs, 'token')]:\n",
    "            for tx in tx_list:\n",
    "                from_addr = tx.get('from', '')\n",
    "                to_addr = tx.get('to', '')\n",
    "                \n",
    "                if from_addr and to_addr:\n",
    "                    key = (from_addr, to_addr)\n",
    "                    if key not in interactions:\n",
    "                        interactions[key] = {'count': 0, 'types': set(), 'values': []}\n",
    "                    \n",
    "                    interactions[key]['count'] += 1\n",
    "                    interactions[key]['types'].add(tx_type)\n",
    "                    \n",
    "                    # Add value information\n",
    "                    if tx_type == 'token' and 'value_scaled' in tx:\n",
    "                        interactions[key]['values'].append(tx['value_scaled'])\n",
    "                    elif tx_type == 'normal' and int(tx.get('value', 0)) > 0:\n",
    "                        interactions[key]['values'].append(int(tx['value']))\n",
    "                    \n",
    "                    all_addresses.update([from_addr, to_addr])\n",
    "        \n",
    "        # Detect suspicious patterns\n",
    "        circular_flows = self.detect_circular_transactions(interactions)\n",
    "        high_interaction_pairs = [(k, v['count']) for k, v in interactions.items() if v['count'] > 50]\n",
    "        multi_type_interactions = [k for k, v in interactions.items() if len(v['types']) > 1]\n",
    "        \n",
    "        return {\n",
    "            \"unique_addresses\": len(all_addresses),\n",
    "            \"interaction_pairs\": len(interactions),\n",
    "            \"circular_flows\": len(circular_flows),\n",
    "            \"high_interaction_pairs\": len(high_interaction_pairs),\n",
    "            \"multi_type_interactions\": len(multi_type_interactions),\n",
    "            \"avg_interactions_per_pair\": sum(v['count'] for v in interactions.values()) / len(interactions) if interactions else 0\n",
    "        }\n",
    "    \n",
    "    def detect_circular_transactions(self, interactions: Dict) -> List[Dict]:\n",
    "        \"\"\"Detect circular transaction patterns (A->B->A)\"\"\"\n",
    "        circular = []\n",
    "        \n",
    "        # Build reverse lookup for efficiency\n",
    "        reverse_interactions = {}\n",
    "        for (from_addr, to_addr), data in interactions.items():\n",
    "            if to_addr not in reverse_interactions:\n",
    "                reverse_interactions[to_addr] = []\n",
    "            reverse_interactions[to_addr].append((from_addr, data))\n",
    "        \n",
    "        # Look for A->B and B->A patterns\n",
    "        for (from_a, to_b), data_ab in interactions.items():\n",
    "            if to_b in reverse_interactions:\n",
    "                for from_b2, data_ba in reverse_interactions[to_b]:\n",
    "                    if from_b2 == from_a:  # Found A->B->A pattern\n",
    "                        circular.append({\n",
    "                            \"address_a\": from_a,\n",
    "                            \"address_b\": to_b,\n",
    "                            \"ab_count\": data_ab['count'],\n",
    "                            \"ba_count\": data_ba['count'],\n",
    "                            \"ab_types\": list(data_ab['types']),\n",
    "                            \"ba_types\": list(data_ba['types'])\n",
    "                        })\n",
    "        \n",
    "        return circular\n",
    "    \n",
    "    def detect_coordinated_patterns(self, normal_txs: List[Dict], internal_txs: List[Dict], token_txs: List[Dict]) -> Dict:\n",
    "        \"\"\"Detect coordinated activity across transaction types\"\"\"\n",
    "        # Group by timestamp\n",
    "        timestamp_activity = defaultdict(lambda: {'types': set(), 'count': 0})\n",
    "        \n",
    "        for tx_list, tx_type in [(normal_txs, 'normal'), (internal_txs, 'internal'), (token_txs, 'token')]:\n",
    "            for tx in tx_list:\n",
    "                timestamp = tx.get('timeStamp', '')\n",
    "                if timestamp:\n",
    "                    timestamp_activity[timestamp]['types'].add(tx_type)\n",
    "                    timestamp_activity[timestamp]['count'] += 1\n",
    "        \n",
    "        # Find coordinated timestamps (multiple transaction types at same time)\n",
    "        coordinated_times = []\n",
    "        for timestamp, activity in timestamp_activity.items():\n",
    "            if len(activity['types']) >= 2:\n",
    "                coordinated_times.append({\n",
    "                    \"timestamp\": timestamp,\n",
    "                    \"types\": list(activity['types']),\n",
    "                    \"transaction_count\": activity['count']\n",
    "                })\n",
    "        \n",
    "        return {\n",
    "            \"coordinated_timestamp_count\": len(coordinated_times),\n",
    "            \"coordination_ratio\": len(coordinated_times) / len(timestamp_activity) if timestamp_activity else 0,\n",
    "            \"total_unique_timestamps\": len(timestamp_activity),\n",
    "            \"coordinated_events\": coordinated_times[:10]  # Sample\n",
    "        }\n",
    "    \n",
    "    # ======================== SCORING SYSTEM ========================\n",
    "    \n",
    "    def calculate_anomaly_score(self, address: str, analysis_results: Dict) -> Dict:\n",
    "        \"\"\"Calculate comprehensive anomaly score\"\"\"\n",
    "        score = 0\n",
    "        flags = []\n",
    "        \n",
    "        # Bot activity (30 points)\n",
    "        if analysis_results.get('bot_patterns', {}).get('bot_suspected', False):\n",
    "            score += 30\n",
    "            flags.append(\"Bot Activity Detected\")\n",
    "        \n",
    "        # High regularity ratio (additional 15 points)\n",
    "        regularity = analysis_results.get('bot_patterns', {}).get('regularity_ratio', 0)\n",
    "        if regularity > 0.7:\n",
    "            score += 15\n",
    "            flags.append(\"High Transaction Regularity\")\n",
    "        \n",
    "        # Round number bias (20 points)\n",
    "        round_ratio = analysis_results.get('value_patterns', {}).get('round_number_ratio', 0)\n",
    "        if round_ratio > 0.5:\n",
    "            score += 20\n",
    "            flags.append(\"Round Number Bias\")\n",
    "        \n",
    "        # Dust attack detection (15 points)\n",
    "        dust_count = analysis_results.get('value_patterns', {}).get('dust_attack_count', 0)\n",
    "        if dust_count > 100:\n",
    "            score += 15\n",
    "            flags.append(\"Potential Dust Attack\")\n",
    "        \n",
    "        # Failed transaction spam (25 points)\n",
    "        failed_ratio = analysis_results.get('gas_patterns', {}).get('failed_tx_ratio', 0)\n",
    "        if failed_ratio > 0.2:\n",
    "            score += 25\n",
    "            flags.append(\"High Failed Transaction Rate\")\n",
    "        \n",
    "        # Coordinated activity (35 points)\n",
    "        coord_ratio = analysis_results.get('coordination_patterns', {}).get('coordination_ratio', 0)\n",
    "        if coord_ratio > 0.3:\n",
    "            score += 35\n",
    "            flags.append(\"Coordinated Multi-Type Transactions\")\n",
    "        \n",
    "        # Transaction frequency spikes (20 points)\n",
    "        spike_count = analysis_results.get('frequency_patterns', {}).get('spike_count', 0)\n",
    "        if spike_count > 3:\n",
    "            score += 20\n",
    "            flags.append(\"Transaction Frequency Spikes\")\n",
    "        \n",
    "        # Circular transaction patterns (40 points)\n",
    "        circular_count = analysis_results.get('relationship_patterns', {}).get('circular_flows', 0)\n",
    "        if circular_count > 0:\n",
    "            score += 40\n",
    "            flags.append(\"Circular Transaction Patterns\")\n",
    "        \n",
    "        # High gas variance - MEV activity (25 points)\n",
    "        gas_variance = analysis_results.get('gas_patterns', {}).get('gas_price_variance', 0)\n",
    "        if gas_variance > 10:\n",
    "            score += 25\n",
    "            flags.append(\"Unusual Gas Price Patterns\")\n",
    "        \n",
    "        # High interaction concentration (20 points)\n",
    "        high_interactions = analysis_results.get('relationship_patterns', {}).get('high_interaction_pairs', 0)\n",
    "        if high_interactions > 5:\n",
    "            score += 20\n",
    "            flags.append(\"High Address Interaction Concentration\")\n",
    "        \n",
    "        return {\n",
    "            \"address\": address,\n",
    "            \"anomaly_score\": min(score, 100),\n",
    "            \"risk_level\": \"HIGH\" if score > 70 else \"MEDIUM\" if score > 40 else \"LOW\",\n",
    "            \"flags\": flags,\n",
    "            \"total_flags\": len(flags)\n",
    "        }\n",
    "    \n",
    "    # ======================== MAIN ANALYSIS FUNCTIONS ========================\n",
    "    \n",
    "    def analyze_address_comprehensive(self, address: str, max_pages=5) -> Dict:\n",
    "        \"\"\"\n",
    "        Comprehensive analysis strategy:\n",
    "        1. Fetch all three transaction types\n",
    "        2. Enrich each type separately \n",
    "        3. Analyze patterns within each type\n",
    "        4. Analyze cross-type relationships\n",
    "        5. Calculate final anomaly score\n",
    "        \"\"\"\n",
    "        print(f\"\\n=== Starting Comprehensive Analysis for {address} ===\")\n",
    "        \n",
    "        # Step 1: Data Collection\n",
    "        print(\"📡 Fetching transaction data...\")\n",
    "        normal_txs = self.get_transactions(address, \"txlist\", max_pages=max_pages)\n",
    "        internal_txs = self.get_transactions(address, \"txlistinternal\", max_pages=max_pages) \n",
    "        token_txs = self.get_transactions(address, \"tokentx\", max_pages=max_pages)\n",
    "        \n",
    "        print(f\"✅ Data collected - Normal: {len(normal_txs)}, Internal: {len(internal_txs)}, Token: {len(token_txs)}\")\n",
    "        \n",
    "        # Step 2: Data Enrichment\n",
    "        print(\"🔍 Enriching transaction data...\")\n",
    "        enriched_normal = self.enrich_normal_transactions(normal_txs)\n",
    "        enriched_internal = self.enrich_internal_transactions(internal_txs)\n",
    "        enriched_token = self.enrich_token_transactions(token_txs)\n",
    "        \n",
    "        # Step 3: Individual Pattern Analysis\n",
    "        print(\"🧠 Analyzing behavioral patterns...\")\n",
    "        \n",
    "        # Combine all transactions for temporal analysis\n",
    "        all_txs = enriched_normal + enriched_internal + enriched_token\n",
    "        \n",
    "        analysis_results = {\n",
    "            # Temporal patterns (all transactions)\n",
    "            \"frequency_patterns\": self.analyze_transaction_frequency(all_txs),\n",
    "            \"bot_patterns\": self.detect_bot_activity(all_txs),\n",
    "            \n",
    "            # Value patterns (normal + token transactions with values)\n",
    "            \"value_patterns\": self.analyze_value_anomalies(\n",
    "                [tx for tx in enriched_normal + enriched_token if \n",
    "                 ('value_scaled' in tx and tx['value_scaled'] > 0) or \n",
    "                 ('value' in tx and int(tx.get('value', 0)) > 0)]\n",
    "            ),\n",
    "            \n",
    "            # Gas patterns (normal transactions only)\n",
    "            \"gas_patterns\": self.analyze_gas_patterns(enriched_normal),\n",
    "            \n",
    "            # Cross-transaction relationships\n",
    "            \"relationship_patterns\": self.analyze_address_relationships(\n",
    "                enriched_normal, enriched_internal, enriched_token\n",
    "            ),\n",
    "            \n",
    "            # Coordination patterns\n",
    "            \"coordination_patterns\": self.detect_coordinated_patterns(\n",
    "                enriched_normal, enriched_internal, enriched_token\n",
    "            )\n",
    "        }\n",
    "        \n",
    "        # Step 4: Calculate Final Score\n",
    "        print(\"📊 Calculating anomaly score...\")\n",
    "        anomaly_result = self.calculate_anomaly_score(address, analysis_results)\n",
    "        \n",
    "        # Step 5: Compile Results\n",
    "        final_results = {\n",
    "            \"address\": address,\n",
    "            \"transaction_summary\": {\n",
    "                \"normal_count\": len(enriched_normal),\n",
    "                \"internal_count\": len(enriched_internal), \n",
    "                \"token_count\": len(enriched_token),\n",
    "                \"total_count\": len(all_txs)\n",
    "            },\n",
    "            \"anomaly_assessment\": anomaly_result,\n",
    "            \"detailed_analysis\": analysis_results,\n",
    "            \"sample_transactions\": {\n",
    "                \"normal\": enriched_normal[:3],\n",
    "                \"internal\": enriched_internal[:3],\n",
    "                \"token\": enriched_token[:3]\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        print(f\"🎯 Analysis Complete! Risk Level: {anomaly_result['risk_level']} (Score: {anomaly_result['anomaly_score']}/100)\")\n",
    "        print(f\"🚩 Flags detected: {', '.join(anomaly_result['flags']) if anomaly_result['flags'] else 'None'}\")\n",
    "        \n",
    "        return final_results\n",
    "\n",
    "# ======================== USAGE EXAMPLES ========================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize detector\n",
    "    API_KEY = \"YOUR_ETHERSCAN_API_KEY\"  # Replace with your actual API key\n",
    "    detector = EtherscanAnomalyDetector(API_KEY)\n",
    "    \n",
    "    # Example 1: Analyze Uniswap V3 Router\n",
    "    uniswap_results = detector.analyze_address_comprehensive(\n",
    "        \"0xE592427A0AEce92De3Edee1F18E0157C05861564\",\n",
    "        max_pages=2  # Reduce for testing\n",
    "    )\n",
    "    \n",
    "    # Print key results\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ANOMALY DETECTION RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Address: {uniswap_results['address']}\")\n",
    "    print(f\"Risk Level: {uniswap_results['anomaly_assessment']['risk_level']}\")\n",
    "    print(f\"Anomaly Score: {uniswap_results['anomaly_assessment']['anomaly_score']}/100\")\n",
    "    print(f\"Flags: {uniswap_results['anomaly_assessment']['flags']}\")\n",
    "    \n",
    "    # Save results to file\n",
    "    with open(f\"anomaly_results_{uniswap_results['address']}.json\", \"w\") as f:\n",
    "        json.dump(uniswap_results, f, indent=2, default=str)\n",
    "    \n",
    "    print(f\"\\n💾 Results saved to anomaly_results_{uniswap_results['address']}.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f1f4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "real time\n",
    "\n",
    "import requests\n",
    "import time\n",
    "import asyncio\n",
    "import threading\n",
    "from datetime import datetime, timedelta\n",
    "from collections import defaultdict, deque\n",
    "from typing import Dict, List, Set, Callable\n",
    "import json\n",
    "\n",
    "class RealTimeEtherscanAnomalyDetector:\n",
    "    def __init__(self, api_key: str):\n",
    "        self.api_key = api_key\n",
    "        self.contract_cache = {}\n",
    "        self.base_url = \"https://api.etherscan.io\"\n",
    "        \n",
    "        # Real-time tracking state\n",
    "        self.monitored_addresses = {}  # {address: last_processed_block}\n",
    "        self.anomaly_callbacks = []    # List of callback functions for anomalies\n",
    "        self.is_monitoring = False\n",
    "        self.monitoring_thread = None\n",
    "        \n",
    "        # Sliding window data for real-time analysis\n",
    "        self.transaction_windows = defaultdict(lambda: {\n",
    "            'normal': deque(maxlen=1000),\n",
    "            'internal': deque(maxlen=1000), \n",
    "            'token': deque(maxlen=1000),\n",
    "            'timestamps': deque(maxlen=1000)\n",
    "        })\n",
    "        \n",
    "        # Real-time anomaly thresholds\n",
    "        self.realtime_thresholds = {\n",
    "            'frequency_spike_multiplier': 5,\n",
    "            'bot_interval_tolerance': 2,  # ±2 seconds\n",
    "            'gas_spike_multiplier': 3,\n",
    "            'coordination_window': 60,    # seconds\n",
    "            'min_txs_for_analysis': 10\n",
    "        }\n",
    "    \n",
    "    # ======================== REAL-TIME DATA FETCHING ========================\n",
    "    \n",
    "    def get_latest_block(self) -> int:\n",
    "        \"\"\"Get the latest block number\"\"\"\n",
    "        url = f\"{self.base_url}/api?module=proxy&action=eth_blockNumber&apikey={self.api_key}\"\n",
    "        try:\n",
    "            response = requests.get(url).json()\n",
    "            return int(response.get('result', '0x0'), 16)\n",
    "        except:\n",
    "            return 0\n",
    "    \n",
    "    def get_transactions_from_block(self, address: str, start_block: int, action: str = \"txlist\") -> List[Dict]:\n",
    "        \"\"\"Fetch transactions from a specific block onwards\"\"\"\n",
    "        url = (\n",
    "            f\"{self.base_url}/v2/api\"\n",
    "            f\"?chainid=1\"\n",
    "            f\"&module=account\"\n",
    "            f\"&action={action}\"\n",
    "            f\"&address={address}\"\n",
    "            f\"&startblock={start_block}\"\n",
    "            f\"&endblock=latest\"\n",
    "            f\"&page=1\"\n",
    "            f\"&offset=100\"  # Smaller batches for real-time\n",
    "            f\"&sort=asc\"    # Ascending to get newest first\n",
    "            f\"&apikey={self.api_key}\"\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url).json()\n",
    "            txs = response.get(\"result\", [])\n",
    "            return txs if isinstance(txs, list) else []\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching {action} from block {start_block}: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def is_contract(self, address: str) -> bool:\n",
    "        \"\"\"Check if address is contract with caching\"\"\"\n",
    "        if address in self.contract_cache:\n",
    "            return self.contract_cache[address]\n",
    "        \n",
    "        url = f\"{self.base_url}/api?module=proxy&action=eth_getCode&address={address}&tag=latest&apikey={self.api_key}\"\n",
    "        try:\n",
    "            response = requests.get(url).json()\n",
    "            result = response.get(\"result\", \"0x\") != \"0x\"\n",
    "            self.contract_cache[address] = result\n",
    "            return result\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    # ======================== REAL-TIME ANOMALY DETECTION ========================\n",
    "    \n",
    "    def detect_realtime_frequency_anomaly(self, address: str, new_txs: List[Dict]) -> Dict:\n",
    "        \"\"\"Detect frequency anomalies in real-time\"\"\"\n",
    "        if not new_txs:\n",
    "            return {\"anomaly\": False}\n",
    "        \n",
    "        window = self.transaction_windows[address]\n",
    "        current_time = int(time.time())\n",
    "        \n",
    "        # Count transactions in last 5 minutes\n",
    "        recent_count = 0\n",
    "        for tx_time in window['timestamps']:\n",
    "            if current_time - tx_time <= 300:  # 5 minutes\n",
    "                recent_count += 1\n",
    "        \n",
    "        new_tx_count = len(new_txs)\n",
    "        \n",
    "        # Anomaly: More than 20 transactions in 5 minutes\n",
    "        if recent_count + new_tx_count > 20:\n",
    "            return {\n",
    "                \"anomaly\": True,\n",
    "                \"type\": \"frequency_spike\",\n",
    "                \"recent_count\": recent_count,\n",
    "                \"new_count\": new_tx_count,\n",
    "                \"severity\": \"HIGH\" if recent_count + new_tx_count > 50 else \"MEDIUM\"\n",
    "            }\n",
    "        \n",
    "        return {\"anomaly\": False}\n",
    "    \n",
    "    def detect_realtime_bot_pattern(self, address: str, new_txs: List[Dict]) -> Dict:\n",
    "        \"\"\"Detect bot patterns in real-time\"\"\"\n",
    "        if len(new_txs) < 3:\n",
    "            return {\"anomaly\": False}\n",
    "        \n",
    "        # Get recent timestamps including new ones\n",
    "        window = self.transaction_windows[address]\n",
    "        all_timestamps = list(window['timestamps']) + [int(tx.get('timeStamp', 0)) for tx in new_txs]\n",
    "        all_timestamps.sort()\n",
    "        \n",
    "        if len(all_timestamps) < 10:\n",
    "            return {\"anomaly\": False}\n",
    "        \n",
    "        # Check last 10 transactions for regular intervals\n",
    "        recent_timestamps = all_timestamps[-10:]\n",
    "        intervals = [recent_timestamps[i+1] - recent_timestamps[i] for i in range(len(recent_timestamps)-1)]\n",
    "        \n",
    "        # Bot pattern: 7+ out of 9 intervals are within 8-12 seconds\n",
    "        regular_intervals = [i for i in intervals if 8 <= i <= 12]\n",
    "        \n",
    "        if len(regular_intervals) >= 7:\n",
    "            return {\n",
    "                \"anomaly\": True,\n",
    "                \"type\": \"bot_pattern\",\n",
    "                \"regular_intervals\": len(regular_intervals),\n",
    "                \"total_intervals\": len(intervals),\n",
    "                \"severity\": \"HIGH\"\n",
    "            }\n",
    "        \n",
    "        return {\"anomaly\": False}\n",
    "    \n",
    "    def detect_realtime_gas_anomaly(self, address: str, new_txs: List[Dict]) -> Dict:\n",
    "        \"\"\"Detect gas anomalies in real-time - only for normal transactions\"\"\"\n",
    "        normal_txs = [tx for tx in new_txs if tx.get('gasPrice')]\n",
    "        if not normal_txs:\n",
    "            return {\"anomaly\": False}\n",
    "        \n",
    "        gas_prices = [int(tx['gasPrice']) for tx in normal_txs]\n",
    "        \n",
    "        # Get baseline from recent transactions\n",
    "        window = self.transaction_windows[address]\n",
    "        recent_normal = [tx for tx in window['normal'] if tx.get('gasPrice')]\n",
    "        \n",
    "        if recent_normal:\n",
    "            baseline_gas = sum(int(tx['gasPrice']) for tx in recent_normal[-50:]) / min(len(recent_normal), 50)\n",
    "            \n",
    "            # Anomaly: New transaction gas > 5x baseline\n",
    "            high_gas_txs = [g for g in gas_prices if g > baseline_gas * 5]\n",
    "            \n",
    "            if high_gas_txs:\n",
    "                return {\n",
    "                    \"anomaly\": True,\n",
    "                    \"type\": \"gas_spike\",\n",
    "                    \"high_gas_count\": len(high_gas_txs),\n",
    "                    \"max_gas\": max(high_gas_txs),\n",
    "                    \"baseline_gas\": baseline_gas,\n",
    "                    \"severity\": \"HIGH\" if max(high_gas_txs) > baseline_gas * 10 else \"MEDIUM\"\n",
    "                }\n",
    "        \n",
    "        return {\"anomaly\": False}\n",
    "    \n",
    "    def detect_realtime_coordination(self, address: str, normal_txs: List[Dict], \n",
    "                                   internal_txs: List[Dict], token_txs: List[Dict]) -> Dict:\n",
    "        \"\"\"Detect coordinated activity across transaction types\"\"\"\n",
    "        if not any([normal_txs, internal_txs, token_txs]):\n",
    "            return {\"anomaly\": False}\n",
    "        \n",
    "        # Check for transactions happening within 60 seconds of each other\n",
    "        all_new_txs = []\n",
    "        for tx_list, tx_type in [(normal_txs, 'normal'), (internal_txs, 'internal'), (token_txs, 'token')]:\n",
    "            for tx in tx_list:\n",
    "                all_new_txs.append((int(tx.get('timeStamp', 0)), tx_type))\n",
    "        \n",
    "        if len(all_new_txs) < 2:\n",
    "            return {\"anomaly\": False}\n",
    "        \n",
    "        all_new_txs.sort()  # Sort by timestamp\n",
    "        \n",
    "        # Look for multiple transaction types within coordination window\n",
    "        coordinated_groups = []\n",
    "        window_start = 0\n",
    "        \n",
    "        for i in range(len(all_new_txs)):\n",
    "            current_time = all_new_txs[i][0]\n",
    "            \n",
    "            # Find all transactions within window\n",
    "            window_txs = []\n",
    "            for j in range(i, len(all_new_txs)):\n",
    "                if all_new_txs[j][0] - current_time <= self.realtime_thresholds['coordination_window']:\n",
    "                    window_txs.append(all_new_txs[j])\n",
    "                else:\n",
    "                    break\n",
    "            \n",
    "            # Check if window has multiple transaction types\n",
    "            types_in_window = set(tx[1] for tx in window_txs)\n",
    "            if len(types_in_window) >= 2 and len(window_txs) >= 3:\n",
    "                coordinated_groups.append({\n",
    "                    \"start_time\": current_time,\n",
    "                    \"types\": list(types_in_window),\n",
    "                    \"transaction_count\": len(window_txs)\n",
    "                })\n",
    "        \n",
    "        if coordinated_groups:\n",
    "            return {\n",
    "                \"anomaly\": True,\n",
    "                \"type\": \"coordination\",\n",
    "                \"coordinated_groups\": len(coordinated_groups),\n",
    "                \"severity\": \"HIGH\" if len(coordinated_groups) > 2 else \"MEDIUM\"\n",
    "            }\n",
    "        \n",
    "        return {\"anomaly\": False}\n",
    "    \n",
    "    # ======================== MONITORING SYSTEM ========================\n",
    "    \n",
    "    def add_address_to_monitor(self, address: str, start_block: int = None):\n",
    "        \"\"\"Add an address to real-time monitoring\"\"\"\n",
    "        if start_block is None:\n",
    "            start_block = self.get_latest_block()\n",
    "        \n",
    "        self.monitored_addresses[address] = start_block\n",
    "        print(f\"📍 Added {address} to monitoring (starting from block {start_block})\")\n",
    "    \n",
    "    def add_anomaly_callback(self, callback: Callable):\n",
    "        \"\"\"Add callback function to be called when anomalies are detected\"\"\"\n",
    "        self.anomaly_callbacks.append(callback)\n",
    "    \n",
    "    def process_new_transactions(self, address: str, normal_txs: List[Dict], \n",
    "                               internal_txs: List[Dict], token_txs: List[Dict]):\n",
    "        \"\"\"Process new transactions and detect anomalies\"\"\"\n",
    "        if not any([normal_txs, internal_txs, token_txs]):\n",
    "            return\n",
    "        \n",
    "        # Update sliding windows\n",
    "        window = self.transaction_windows[address]\n",
    "        current_time = int(time.time())\n",
    "        \n",
    "        # Add to windows\n",
    "        for tx in normal_txs:\n",
    "            window['normal'].append(tx)\n",
    "            window['timestamps'].append(int(tx.get('timeStamp', current_time)))\n",
    "        \n",
    "        for tx in internal_txs:\n",
    "            window['internal'].append(tx)\n",
    "            window['timestamps'].append(int(tx.get('timeStamp', current_time)))\n",
    "        \n",
    "        for tx in token_txs:\n",
    "            window['token'].append(tx)\n",
    "            window['timestamps'].append(int(tx.get('timeStamp', current_time)))\n",
    "        \n",
    "        # Run real-time anomaly detection\n",
    "        all_new_txs = normal_txs + internal_txs + token_txs\n",
    "        anomalies_detected = []\n",
    "        \n",
    "        # Frequency anomaly\n",
    "        freq_anomaly = self.detect_realtime_frequency_anomaly(address, all_new_txs)\n",
    "        if freq_anomaly['anomaly']:\n",
    "            anomalies_detected.append(freq_anomaly)\n",
    "        \n",
    "        # Bot pattern\n",
    "        bot_anomaly = self.detect_realtime_bot_pattern(address, all_new_txs)\n",
    "        if bot_anomaly['anomaly']:\n",
    "            anomalies_detected.append(bot_anomaly)\n",
    "        \n",
    "        # Gas anomaly\n",
    "        gas_anomaly = self.detect_realtime_gas_anomaly(address, normal_txs)\n",
    "        if gas_anomaly['anomaly']:\n",
    "            anomalies_detected.append(gas_anomaly)\n",
    "        \n",
    "        # Coordination anomaly\n",
    "        coord_anomaly = self.detect_realtime_coordination(address, normal_txs, internal_txs, token_txs)\n",
    "        if coord_anomaly['anomaly']:\n",
    "            anomalies_detected.append(coord_anomaly)\n",
    "        \n",
    "        # Trigger callbacks if anomalies detected\n",
    "        if anomalies_detected:\n",
    "            anomaly_event = {\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                \"address\": address,\n",
    "                \"new_transactions\": {\n",
    "                    \"normal\": len(normal_txs),\n",
    "                    \"internal\": len(internal_txs),\n",
    "                    \"token\": len(token_txs)\n",
    "                },\n",
    "                \"anomalies\": anomalies_detected\n",
    "            }\n",
    "            \n",
    "            for callback in self.anomaly_callbacks:\n",
    "                try:\n",
    "                    callback(anomaly_event)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error in anomaly callback: {e}\")\n",
    "    \n",
    "    def monitoring_loop(self):\n",
    "        \"\"\"Main monitoring loop - runs in background thread\"\"\"\n",
    "        print(\"🚀 Starting real-time monitoring...\")\n",
    "        \n",
    "        while self.is_monitoring:\n",
    "            for address, last_block in list(self.monitored_addresses.items()):\n",
    "                try:\n",
    "                    # Fetch new transactions from last processed block\n",
    "                    normal_txs = self.get_transactions_from_block(address, last_block + 1, \"txlist\")\n",
    "                    internal_txs = self.get_transactions_from_block(address, last_block + 1, \"txlistinternal\")\n",
    "                    token_txs = self.get_transactions_from_block(address, last_block + 1, \"tokentx\")\n",
    "                    \n",
    "                    # Process if we have new transactions\n",
    "                    if any([normal_txs, internal_txs, token_txs]):\n",
    "                        print(f\"🔄 Processing {len(normal_txs + internal_txs + token_txs)} new transactions for {address}\")\n",
    "                        \n",
    "                        # Enrich transactions\n",
    "                        for tx in normal_txs + internal_txs + token_txs:\n",
    "                            if 'from' in tx and tx['from']:\n",
    "                                tx['from_type'] = \"Contract\" if self.is_contract(tx['from']) else \"EOA\"\n",
    "                            if 'to' in tx and tx['to']:\n",
    "                                tx['to_type'] = \"Contract\" if self.is_contract(tx['to']) else \"EOA\"\n",
    "                        \n",
    "                        # Process for anomalies\n",
    "                        self.process_new_transactions(address, normal_txs, internal_txs, token_txs)\n",
    "                        \n",
    "                        # Update last processed block\n",
    "                        latest_block = self.get_latest_block()\n",
    "                        self.monitored_addresses[address] = latest_block\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error monitoring {address}: {e}\")\n",
    "                \n",
    "                # Rate limiting\n",
    "                time.sleep(1)  # 1 second between addresses\n",
    "            \n",
    "            # Wait before next monitoring cycle\n",
    "            time.sleep(30)  # Check every 30 seconds\n",
    "    \n",
    "    def start_monitoring(self):\n",
    "        \"\"\"Start real-time monitoring in background thread\"\"\"\n",
    "        if self.is_monitoring:\n",
    "            print(\"⚠️ Monitoring already running\")\n",
    "            return\n",
    "        \n",
    "        self.is_monitoring = True\n",
    "        self.monitoring_thread = threading.Thread(target=self.monitoring_loop, daemon=True)\n",
    "        self.monitoring_thread.start()\n",
    "        print(\"✅ Real-time monitoring started\")\n",
    "    \n",
    "    def stop_monitoring(self):\n",
    "        \"\"\"Stop real-time monitoring\"\"\"\n",
    "        self.is_monitoring = False\n",
    "        if self.monitoring_thread:\n",
    "            self.monitoring_thread.join()\n",
    "        print(\"🛑 Real-time monitoring stopped\")\n",
    "    \n",
    "    def get_monitoring_status(self) -> Dict:\n",
    "        \"\"\"Get current monitoring status\"\"\"\n",
    "        return {\n",
    "            \"is_monitoring\": self.is_monitoring,\n",
    "            \"monitored_addresses\": len(self.monitored_addresses),\n",
    "            \"addresses\": list(self.monitored_addresses.keys()),\n",
    "            \"callback_count\": len(self.anomaly_callbacks)\n",
    "        }\n",
    "\n",
    "# ======================== CALLBACK EXAMPLES ========================\n",
    "\n",
    "def alert_callback(anomaly_event):\n",
    "    \"\"\"Example callback that prints alerts\"\"\"\n",
    "    address = anomaly_event['address']\n",
    "    anomalies = anomaly_event['anomalies']\n",
    "    \n",
    "    print(f\"\\n🚨 ANOMALY ALERT for {address}\")\n",
    "    print(f\"Time: {anomaly_event['timestamp']}\")\n",
    "    \n",
    "    for anomaly in anomalies:\n",
    "        print(f\"  ⚠️ {anomaly['type'].upper()}: {anomaly.get('severity', 'MEDIUM')} severity\")\n",
    "    \n",
    "    print(f\"  📊 New transactions: {anomaly_event['new_transactions']}\")\n",
    "\n",
    "def log_callback(anomaly_event):\n",
    "    \"\"\"Example callback that logs to file\"\"\"\n",
    "    with open(\"realtime_anomalies.log\", \"a\") as f:\n",
    "        f.write(json.dumps(anomaly_event, indent=2) + \"\\n\")\n",
    "\n",
    "# ======================== USAGE EXAMPLE ========================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize real-time detector\n",
    "    API_KEY = \"YOUR_ETHERSCAN_API_KEY\"\n",
    "    detector = RealTimeEtherscanAnomalyDetector(API_KEY)\n",
    "    \n",
    "    # Add callback functions\n",
    "    detector.add_anomaly_callback(alert_callback)\n",
    "    detector.add_anomaly_callback(log_callback)\n",
    "    \n",
    "    # Add addresses to monitor\n",
    "    detector.add_address_to_monitor(\"0xE592427A0AEce92De3Edee1F18E0157C05861564\")  # Uniswap V3\n",
    "    detector.add_address_to_monitor(\"0x7a250d5630B4cF539739dF2C5dAcb4c659F2488D\")  # Uniswap V2\n",
    "    \n",
    "    # Start monitoring\n",
    "    detector.start_monitoring()\n",
    "    \n",
    "    try:\n",
    "        # Keep the script running\n",
    "        while True:\n",
    "            status = detector.get_monitoring_status()\n",
    "            print(f\"📊 Monitoring {status['monitored_addresses']} addresses...\")\n",
    "            time.sleep(60)  # Print status every minute\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n🛑 Stopping monitoring...\")\n",
    "        detector.stop_monitoring()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4906295c",
   "metadata": {},
   "outputs": [],
   "source": [
    "include all endpoints\n",
    "\n",
    "import requests\n",
    "import time\n",
    "import asyncio\n",
    "import threading\n",
    "from datetime import datetime, timedelta\n",
    "from collections import defaultdict, deque\n",
    "from typing import Dict, List, Set, Callable\n",
    "import json\n",
    "\n",
    "class RealTimeEtherscanAnomalyDetector:\n",
    "    def __init__(self, api_key: str):\n",
    "        self.api_key = api_key\n",
    "        self.contract_cache = {}\n",
    "        self.base_url = \"https://api.etherscan.io\"\n",
    "        \n",
    "        # Real-time tracking state - separate tracking for each transaction type\n",
    "        self.monitored_addresses = {}  # {address: {txlist: block, txlistinternal: block, tokentx: block}}\n",
    "        self.anomaly_callbacks = []    # List of callback functions for anomalies\n",
    "        self.is_monitoring = False\n",
    "        self.monitoring_thread = None\n",
    "        \n",
    "        # Sliding window data for real-time analysis\n",
    "        self.transaction_windows = defaultdict(lambda: {\n",
    "            'normal': deque(maxlen=1000),\n",
    "            'internal': deque(maxlen=1000), \n",
    "            'token': deque(maxlen=1000),\n",
    "            'timestamps': deque(maxlen=1000)\n",
    "        })\n",
    "        \n",
    "        # Real-time anomaly thresholds\n",
    "        self.realtime_thresholds = {\n",
    "            'frequency_spike_multiplier': 5,\n",
    "            'bot_interval_tolerance': 2,  # ±2 seconds\n",
    "            'gas_spike_multiplier': 3,\n",
    "            'coordination_window': 60,    # seconds\n",
    "            'min_txs_for_analysis': 10\n",
    "        }\n",
    "    \n",
    "    # ======================== REAL-TIME DATA FETCHING ========================\n",
    "    \n",
    "    def get_latest_block(self) -> int:\n",
    "        \"\"\"Get the latest block number\"\"\"\n",
    "        url = f\"{self.base_url}/api?module=proxy&action=eth_blockNumber&apikey={self.api_key}\"\n",
    "        try:\n",
    "            response = requests.get(url).json()\n",
    "            return int(response.get('result', '0x0'), 16)\n",
    "        except:\n",
    "            return 0\n",
    "    \n",
    "    def get_transactions_since_block(self, address: str, start_block: int, action: str) -> List[Dict]:\n",
    "        \"\"\"Fetch transactions from a specific block onwards for any transaction type\"\"\"\n",
    "        # Different API endpoints for different transaction types\n",
    "        if action == \"txlist\":\n",
    "            module = \"account\"\n",
    "        elif action == \"txlistinternal\":\n",
    "            module = \"account\" \n",
    "        elif action == \"tokentx\":\n",
    "            module = \"account\"\n",
    "        else:\n",
    "            return []\n",
    "            \n",
    "        url = (\n",
    "            f\"{self.base_url}/v2/api\"\n",
    "            f\"?chainid=1\"\n",
    "            f\"&module={module}\"\n",
    "            f\"&action={action}\"\n",
    "            f\"&address={address}\"\n",
    "            f\"&startblock={start_block}\"\n",
    "            f\"&endblock=latest\"\n",
    "            f\"&page=1\"\n",
    "            f\"&offset=100\"  # Smaller batches for real-time\n",
    "            f\"&sort=asc\"    # Ascending to get chronological order\n",
    "            f\"&apikey={self.api_key}\"\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url).json()\n",
    "            txs = response.get(\"result\", [])\n",
    "            \n",
    "            # Handle different response formats\n",
    "            if isinstance(txs, list):\n",
    "                return txs\n",
    "            elif txs == \"No transactions found\":\n",
    "                return []\n",
    "            else:\n",
    "                return []\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching {action} from block {start_block}: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def get_highest_block_from_txs(self, txs: List[Dict]) -> int:\n",
    "        \"\"\"Get the highest block number from a list of transactions\"\"\"\n",
    "        if not txs:\n",
    "            return 0\n",
    "        \n",
    "        block_numbers = []\n",
    "        for tx in txs:\n",
    "            block_num = tx.get('blockNumber', '0')\n",
    "            try:\n",
    "                block_numbers.append(int(block_num))\n",
    "            except (ValueError, TypeError):\n",
    "                continue\n",
    "        \n",
    "        return max(block_numbers) if block_numbers else 0\n",
    "    \n",
    "    def is_contract(self, address: str) -> bool:\n",
    "        \"\"\"Check if address is contract with caching\"\"\"\n",
    "        if address in self.contract_cache:\n",
    "            return self.contract_cache[address]\n",
    "        \n",
    "        url = f\"{self.base_url}/api?module=proxy&action=eth_getCode&address={address}&tag=latest&apikey={self.api_key}\"\n",
    "        try:\n",
    "            response = requests.get(url).json()\n",
    "            result = response.get(\"result\", \"0x\") != \"0x\"\n",
    "            self.contract_cache[address] = result\n",
    "            return result\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    # ======================== REAL-TIME ANOMALY DETECTION ========================\n",
    "    \n",
    "    def detect_realtime_frequency_anomaly(self, address: str, new_txs: List[Dict]) -> Dict:\n",
    "        \"\"\"Detect frequency anomalies in real-time\"\"\"\n",
    "        if not new_txs:\n",
    "            return {\"anomaly\": False}\n",
    "        \n",
    "        window = self.transaction_windows[address]\n",
    "        current_time = int(time.time())\n",
    "        \n",
    "        # Count transactions in last 5 minutes\n",
    "        recent_count = 0\n",
    "        for tx_time in window['timestamps']:\n",
    "            if current_time - tx_time <= 300:  # 5 minutes\n",
    "                recent_count += 1\n",
    "        \n",
    "        new_tx_count = len(new_txs)\n",
    "        \n",
    "        # Anomaly: More than 20 transactions in 5 minutes\n",
    "        if recent_count + new_tx_count > 20:\n",
    "            return {\n",
    "                \"anomaly\": True,\n",
    "                \"type\": \"frequency_spike\",\n",
    "                \"recent_count\": recent_count,\n",
    "                \"new_count\": new_tx_count,\n",
    "                \"severity\": \"HIGH\" if recent_count + new_tx_count > 50 else \"MEDIUM\"\n",
    "            }\n",
    "        \n",
    "        return {\"anomaly\": False}\n",
    "    \n",
    "    def detect_realtime_bot_pattern(self, address: str, new_txs: List[Dict]) -> Dict:\n",
    "        \"\"\"Detect bot patterns in real-time\"\"\"\n",
    "        if len(new_txs) < 3:\n",
    "            return {\"anomaly\": False}\n",
    "        \n",
    "        # Get recent timestamps including new ones\n",
    "        window = self.transaction_windows[address]\n",
    "        all_timestamps = list(window['timestamps']) + [int(tx.get('timeStamp', 0)) for tx in new_txs]\n",
    "        all_timestamps.sort()\n",
    "        \n",
    "        if len(all_timestamps) < 10:\n",
    "            return {\"anomaly\": False}\n",
    "        \n",
    "        # Check last 10 transactions for regular intervals\n",
    "        recent_timestamps = all_timestamps[-10:]\n",
    "        intervals = [recent_timestamps[i+1] - recent_timestamps[i] for i in range(len(recent_timestamps)-1)]\n",
    "        \n",
    "        # Bot pattern: 7+ out of 9 intervals are within 8-12 seconds\n",
    "        regular_intervals = [i for i in intervals if 8 <= i <= 12]\n",
    "        \n",
    "        if len(regular_intervals) >= 7:\n",
    "            return {\n",
    "                \"anomaly\": True,\n",
    "                \"type\": \"bot_pattern\",\n",
    "                \"regular_intervals\": len(regular_intervals),\n",
    "                \"total_intervals\": len(intervals),\n",
    "                \"severity\": \"HIGH\"\n",
    "            }\n",
    "        \n",
    "        return {\"anomaly\": False}\n",
    "    \n",
    "    def detect_realtime_gas_anomaly(self, address: str, new_txs: List[Dict]) -> Dict:\n",
    "        \"\"\"Detect gas anomalies in real-time - only for normal transactions\"\"\"\n",
    "        normal_txs = [tx for tx in new_txs if tx.get('gasPrice')]\n",
    "        if not normal_txs:\n",
    "            return {\"anomaly\": False}\n",
    "        \n",
    "        gas_prices = [int(tx['gasPrice']) for tx in normal_txs]\n",
    "        \n",
    "        # Get baseline from recent transactions\n",
    "        window = self.transaction_windows[address]\n",
    "        recent_normal = [tx for tx in window['normal'] if tx.get('gasPrice')]\n",
    "        \n",
    "        if recent_normal:\n",
    "            baseline_gas = sum(int(tx['gasPrice']) for tx in recent_normal[-50:]) / min(len(recent_normal), 50)\n",
    "            \n",
    "            # Anomaly: New transaction gas > 5x baseline\n",
    "            high_gas_txs = [g for g in gas_prices if g > baseline_gas * 5]\n",
    "            \n",
    "            if high_gas_txs:\n",
    "                return {\n",
    "                    \"anomaly\": True,\n",
    "                    \"type\": \"gas_spike\",\n",
    "                    \"high_gas_count\": len(high_gas_txs),\n",
    "                    \"max_gas\": max(high_gas_txs),\n",
    "                    \"baseline_gas\": baseline_gas,\n",
    "                    \"severity\": \"HIGH\" if max(high_gas_txs) > baseline_gas * 10 else \"MEDIUM\"\n",
    "                }\n",
    "        \n",
    "        return {\"anomaly\": False}\n",
    "    \n",
    "    def detect_realtime_coordination(self, address: str, normal_txs: List[Dict], \n",
    "                                   internal_txs: List[Dict], token_txs: List[Dict]) -> Dict:\n",
    "        \"\"\"Detect coordinated activity across transaction types\"\"\"\n",
    "        if not any([normal_txs, internal_txs, token_txs]):\n",
    "            return {\"anomaly\": False}\n",
    "        \n",
    "        # Check for transactions happening within 60 seconds of each other\n",
    "        all_new_txs = []\n",
    "        for tx_list, tx_type in [(normal_txs, 'normal'), (internal_txs, 'internal'), (token_txs, 'token')]:\n",
    "            for tx in tx_list:\n",
    "                all_new_txs.append((int(tx.get('timeStamp', 0)), tx_type))\n",
    "        \n",
    "        if len(all_new_txs) < 2:\n",
    "            return {\"anomaly\": False}\n",
    "        \n",
    "        all_new_txs.sort()  # Sort by timestamp\n",
    "        \n",
    "        # Look for multiple transaction types within coordination window\n",
    "        coordinated_groups = []\n",
    "        window_start = 0\n",
    "        \n",
    "        for i in range(len(all_new_txs)):\n",
    "            current_time = all_new_txs[i][0]\n",
    "            \n",
    "            # Find all transactions within window\n",
    "            window_txs = []\n",
    "            for j in range(i, len(all_new_txs)):\n",
    "                if all_new_txs[j][0] - current_time <= self.realtime_thresholds['coordination_window']:\n",
    "                    window_txs.append(all_new_txs[j])\n",
    "                else:\n",
    "                    break\n",
    "            \n",
    "            # Check if window has multiple transaction types\n",
    "            types_in_window = set(tx[1] for tx in window_txs)\n",
    "            if len(types_in_window) >= 2 and len(window_txs) >= 3:\n",
    "                coordinated_groups.append({\n",
    "                    \"start_time\": current_time,\n",
    "                    \"types\": list(types_in_window),\n",
    "                    \"transaction_count\": len(window_txs)\n",
    "                })\n",
    "        \n",
    "        if coordinated_groups:\n",
    "            return {\n",
    "                \"anomaly\": True,\n",
    "                \"type\": \"coordination\",\n",
    "                \"coordinated_groups\": len(coordinated_groups),\n",
    "                \"severity\": \"HIGH\" if len(coordinated_groups) > 2 else \"MEDIUM\"\n",
    "            }\n",
    "        \n",
    "        return {\"anomaly\": False}\n",
    "    \n",
    "    # ======================== MONITORING SYSTEM ========================\n",
    "    \n",
    "    def add_address_to_monitor(self, address: str, start_block: int = None):\n",
    "        \"\"\"Add an address to real-time monitoring with separate tracking for each tx type\"\"\"\n",
    "        if start_block is None:\n",
    "            start_block = self.get_latest_block()\n",
    "        \n",
    "        # Initialize tracking for all three transaction types\n",
    "        self.monitored_addresses[address] = {\n",
    "            'txlist': start_block,\n",
    "            'txlistinternal': start_block,\n",
    "            'tokentx': start_block\n",
    "        }\n",
    "        print(f\"📍 Added {address} to monitoring (starting from block {start_block})\")\n",
    "        print(f\"   Tracking: normal, internal, and token transactions\")\n",
    "    \n",
    "    def add_anomaly_callback(self, callback: Callable):\n",
    "        \"\"\"Add callback function to be called when anomalies are detected\"\"\"\n",
    "        self.anomaly_callbacks.append(callback)\n",
    "    \n",
    "    def process_new_transactions(self, address: str, normal_txs: List[Dict], \n",
    "                               internal_txs: List[Dict], token_txs: List[Dict]):\n",
    "        \"\"\"Process new transactions and detect anomalies\"\"\"\n",
    "        if not any([normal_txs, internal_txs, token_txs]):\n",
    "            return\n",
    "        \n",
    "        # Update sliding windows\n",
    "        window = self.transaction_windows[address]\n",
    "        current_time = int(time.time())\n",
    "        \n",
    "        # Add to windows\n",
    "        for tx in normal_txs:\n",
    "            window['normal'].append(tx)\n",
    "            window['timestamps'].append(int(tx.get('timeStamp', current_time)))\n",
    "        \n",
    "        for tx in internal_txs:\n",
    "            window['internal'].append(tx)\n",
    "            window['timestamps'].append(int(tx.get('timeStamp', current_time)))\n",
    "        \n",
    "        for tx in token_txs:\n",
    "            window['token'].append(tx)\n",
    "            window['timestamps'].append(int(tx.get('timeStamp', current_time)))\n",
    "        \n",
    "        # Run real-time anomaly detection\n",
    "        all_new_txs = normal_txs + internal_txs + token_txs\n",
    "        anomalies_detected = []\n",
    "        \n",
    "        # Frequency anomaly\n",
    "        freq_anomaly = self.detect_realtime_frequency_anomaly(address, all_new_txs)\n",
    "        if freq_anomaly['anomaly']:\n",
    "            anomalies_detected.append(freq_anomaly)\n",
    "        \n",
    "        # Bot pattern\n",
    "        bot_anomaly = self.detect_realtime_bot_pattern(address, all_new_txs)\n",
    "        if bot_anomaly['anomaly']:\n",
    "            anomalies_detected.append(bot_anomaly)\n",
    "        \n",
    "        # Gas anomaly\n",
    "        gas_anomaly = self.detect_realtime_gas_anomaly(address, normal_txs)\n",
    "        if gas_anomaly['anomaly']:\n",
    "            anomalies_detected.append(gas_anomaly)\n",
    "        \n",
    "        # Coordination anomaly\n",
    "        coord_anomaly = self.detect_realtime_coordination(address, normal_txs, internal_txs, token_txs)\n",
    "        if coord_anomaly['anomaly']:\n",
    "            anomalies_detected.append(coord_anomaly)\n",
    "        \n",
    "        # Trigger callbacks if anomalies detected\n",
    "        if anomalies_detected:\n",
    "            anomaly_event = {\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                \"address\": address,\n",
    "                \"new_transactions\": {\n",
    "                    \"normal\": len(normal_txs),\n",
    "                    \"internal\": len(internal_txs),\n",
    "                    \"token\": len(token_txs)\n",
    "                },\n",
    "                \"anomalies\": anomalies_detected\n",
    "            }\n",
    "            \n",
    "            for callback in self.anomaly_callbacks:\n",
    "                try:\n",
    "                    callback(anomaly_event)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error in anomaly callback: {e}\")\n",
    "    \n",
    "    def monitoring_loop(self):\n",
    "        \"\"\"Main monitoring loop - runs in background thread\"\"\"\n",
    "        print(\"🚀 Starting real-time monitoring for all transaction types...\")\n",
    "        \n",
    "        while self.is_monitoring:\n",
    "            for address, tracking_info in list(self.monitored_addresses.items()):\n",
    "                try:\n",
    "                    new_normal_txs = []\n",
    "                    new_internal_txs = []\n",
    "                    new_token_txs = []\n",
    "                    \n",
    "                    # Fetch new transactions for each type separately\n",
    "                    print(f\"🔍 Checking {address} for new transactions...\")\n",
    "                    \n",
    "                    # Normal transactions (txlist)\n",
    "                    normal_start_block = tracking_info['txlist'] + 1\n",
    "                    raw_normal = self.get_transactions_since_block(address, normal_start_block, \"txlist\")\n",
    "                    if raw_normal:\n",
    "                        new_normal_txs = self.enrich_transactions_realtime(raw_normal, 'normal')\n",
    "                        # Update tracking\n",
    "                        highest_normal_block = self.get_highest_block_from_txs(raw_normal)\n",
    "                        if highest_normal_block > 0:\n",
    "                            tracking_info['txlist'] = highest_normal_block\n",
    "                        print(f\"   📥 {len(new_normal_txs)} new normal transactions\")\n",
    "                    \n",
    "                    # Internal transactions (txlistinternal)  \n",
    "                    internal_start_block = tracking_info['txlistinternal'] + 1\n",
    "                    raw_internal = self.get_transactions_since_block(address, internal_start_block, \"txlistinternal\")\n",
    "                    if raw_internal:\n",
    "                        new_internal_txs = self.enrich_transactions_realtime(raw_internal, 'internal')\n",
    "                        # Update tracking\n",
    "                        highest_internal_block = self.get_highest_block_from_txs(raw_internal)\n",
    "                        if highest_internal_block > 0:\n",
    "                            tracking_info['txlistinternal'] = highest_internal_block\n",
    "                        print(f\"   📥 {len(new_internal_txs)} new internal transactions\")\n",
    "                    \n",
    "                    # Token transactions (tokentx)\n",
    "                    token_start_block = tracking_info['tokentx'] + 1\n",
    "                    raw_token = self.get_transactions_since_block(address, token_start_block, \"tokentx\")\n",
    "                    if raw_token:\n",
    "                        new_token_txs = self.enrich_transactions_realtime(raw_token, 'token')\n",
    "                        # Update tracking\n",
    "                        highest_token_block = self.get_highest_block_from_txs(raw_token)\n",
    "                        if highest_token_block > 0:\n",
    "                            tracking_info['tokentx'] = highest_token_block\n",
    "                        print(f\"   📥 {len(new_token_txs)} new token transactions\")\n",
    "                    \n",
    "                    # Process all new transactions if we have any\n",
    "                    if any([new_normal_txs, new_internal_txs, new_token_txs]):\n",
    "                        total_new = len(new_normal_txs) + len(new_internal_txs) + len(new_token_txs)\n",
    "                        print(f\"🔄 Processing {total_new} total new transactions for {address}\")\n",
    "                        \n",
    "                        # Process for anomalies\n",
    "                        self.process_new_transactions(address, new_normal_txs, new_internal_txs, new_token_txs)\n",
    "                    else:\n",
    "                        print(f\"   ✅ No new transactions for {address}\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"❌ Error monitoring {address}: {e}\")\n",
    "                \n",
    "                # Rate limiting between addresses\n",
    "                time.sleep(2)  # 2 seconds between addresses to respect API limits\n",
    "            \n",
    "            # Wait before next monitoring cycle\n",
    "            print(f\"⏱️ Monitoring cycle complete. Waiting 45 seconds before next cycle...\")\n",
    "            time.sleep(45)  # Check every 45 seconds\n",
    "    \n",
    "    def start_monitoring(self):\n",
    "        \"\"\"Start real-time monitoring in background thread\"\"\"\n",
    "        if self.is_monitoring:\n",
    "            print(\"⚠️ Monitoring already running\")\n",
    "            return\n",
    "        \n",
    "        self.is_monitoring = True\n",
    "        self.monitoring_thread = threading.Thread(target=self.monitoring_loop, daemon=True)\n",
    "        self.monitoring_thread.start()\n",
    "        print(\"✅ Real-time monitoring started\")\n",
    "    \n",
    "    def stop_monitoring(self):\n",
    "        \"\"\"Stop real-time monitoring\"\"\"\n",
    "        self.is_monitoring = False\n",
    "        if self.monitoring_thread:\n",
    "            self.monitoring_thread.join()\n",
    "        print(\"🛑 Real-time monitoring stopped\")\n",
    "    \n",
    "    def get_monitoring_status(self) -> Dict:\n",
    "        \"\"\"Get current monitoring status\"\"\"\n",
    "        return {\n",
    "            \"is_monitoring\": self.is_monitoring,\n",
    "            \"monitored_addresses\": len(self.monitored_addresses),\n",
    "            \"addresses\": list(self.monitored_addresses.keys()),\n",
    "            \"callback_count\": len(self.anomaly_callbacks)\n",
    "        }\n",
    "\n",
    "# ======================== CALLBACK EXAMPLES ========================\n",
    "\n",
    "def alert_callback(anomaly_event):\n",
    "    \"\"\"Example callback that prints alerts\"\"\"\n",
    "    address = anomaly_event['address']\n",
    "    anomalies = anomaly_event['anomalies']\n",
    "    \n",
    "    print(f\"\\n🚨 ANOMALY ALERT for {address}\")\n",
    "    print(f\"Time: {anomaly_event['timestamp']}\")\n",
    "    \n",
    "    for anomaly in anomalies:\n",
    "        print(f\"  ⚠️ {anomaly['type'].upper()}: {anomaly.get('severity', 'MEDIUM')} severity\")\n",
    "    \n",
    "    print(f\"  📊 New transactions: {anomaly_event['new_transactions']}\")\n",
    "\n",
    "def log_callback(anomaly_event):\n",
    "    \"\"\"Example callback that logs to file\"\"\"\n",
    "    with open(\"realtime_anomalies.log\", \"a\") as f:\n",
    "        f.write(json.dumps(anomaly_event, indent=2) + \"\\n\")\n",
    "\n",
    "# ======================== USAGE EXAMPLE ========================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize real-time detector\n",
    "    API_KEY = \"YOUR_ETHERSCAN_API_KEY\"\n",
    "    detector = RealTimeEtherscanAnomalyDetector(API_KEY)\n",
    "    \n",
    "    # Add callback functions\n",
    "    detector.add_anomaly_callback(alert_callback)\n",
    "    detector.add_anomaly_callback(log_callback)\n",
    "    \n",
    "    # Add addresses to monitor\n",
    "    detector.add_address_to_monitor(\"0xE592427A0AEce92De3Edee1F18E0157C05861564\")  # Uniswap V3\n",
    "    detector.add_address_to_monitor(\"0x7a250d5630B4cF539739dF2C5dAcb4c659F2488D\")  # Uniswap V2\n",
    "    \n",
    "    # Start monitoring\n",
    "    detector.start_monitoring()\n",
    "    \n",
    "    try:\n",
    "        # Keep the script running\n",
    "        while True:\n",
    "            status = detector.get_monitoring_status()\n",
    "            print(f\"📊 Monitoring {status['monitored_addresses']} addresses...\")\n",
    "            time.sleep(60)  # Print status every minute\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n🛑 Stopping monitoring...\")\n",
    "        detector.stop_monitoring()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d29c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import asyncio\n",
    "import threading\n",
    "from datetime import datetime, timedelta\n",
    "from collections import defaultdict, deque\n",
    "from typing import Dict, List, Set, Callable, Optional\n",
    "import json\n",
    "\n",
    "class ReactiveEtherscanAnomalyDetector:\n",
    "    def __init__(self, api_key: str):\n",
    "        self.api_key = api_key\n",
    "        self.base_url = \"https://api.etherscan.io\"\n",
    "        \n",
    "        # Caching system\n",
    "        self.contract_cache = {}  # {address: bool}\n",
    "        self.address_cache = {}   # {address: {last_check: timestamp, data: recent_txs}}\n",
    "        self.cache_ttl = 300      # 5 minutes cache TTL\n",
    "        \n",
    "        # Rate limiting\n",
    "        self.last_request_time = 0\n",
    "        self.min_request_interval = 0.2  # 200ms between requests (5 req/sec max)\n",
    "        self.daily_request_count = 0\n",
    "        self.daily_limit = 100000  # Etherscan daily limit\n",
    "        \n",
    "        # Reactive monitoring state\n",
    "        self.anomaly_triggers = []  # List of trigger functions\n",
    "        self.alert_callbacks = []\n",
    "        self.monitoring_window_hours = 48  # Focus on last 48 hours\n",
    "        \n",
    "        # Quick check system (lightweight monitoring)\n",
    "        self.quick_check_addresses = set()\n",
    "        self.is_quick_monitoring = False\n",
    "        self.quick_monitor_thread = None\n",
    "        \n",
    "    # ======================== RATE LIMITING & CACHING ========================\n",
    "    \n",
    "    def _rate_limit(self):\n",
    "        \"\"\"Enforce rate limiting\"\"\"\n",
    "        current_time = time.time()\n",
    "        time_since_last = current_time - self.last_request_time\n",
    "        \n",
    "        if time_since_last < self.min_request_interval:\n",
    "            sleep_time = self.min_request_interval - time_since_last\n",
    "            time.sleep(sleep_time)\n",
    "        \n",
    "        self.last_request_time = time.time()\n",
    "        self.daily_request_count += 1\n",
    "        \n",
    "        if self.daily_request_count >= self.daily_limit:\n",
    "            raise Exception(\"Daily API limit reached\")\n",
    "    \n",
    "    def _get_cache_key(self, address: str, action: str) -> str:\n",
    "        \"\"\"Generate cache key for address and action\"\"\"\n",
    "        return f\"{address}:{action}\"\n",
    "    \n",
    "    def _is_cache_valid(self, address: str, action: str) -> bool:\n",
    "        \"\"\"Check if cached data is still valid\"\"\"\n",
    "        cache_key = self._get_cache_key(address, action)\n",
    "        if cache_key not in self.address_cache:\n",
    "            return False\n",
    "        \n",
    "        cached_data = self.address_cache[cache_key]\n",
    "        cache_age = time.time() - cached_data['last_check']\n",
    "        return cache_age < self.cache_ttl\n",
    "    \n",
    "    def _get_cached_data(self, address: str, action: str) -> Optional[List[Dict]]:\n",
    "        \"\"\"Get cached data if valid\"\"\"\n",
    "        if self._is_cache_valid(address, action):\n",
    "            cache_key = self._get_cache_key(address, action)\n",
    "            return self.address_cache[cache_key]['data']\n",
    "        return None\n",
    "    \n",
    "    def _cache_data(self, address: str, action: str, data: List[Dict]):\n",
    "        \"\"\"Cache transaction data\"\"\"\n",
    "        cache_key = self._get_cache_key(address, action)\n",
    "        self.address_cache[cache_key] = {\n",
    "            'last_check': time.time(),\n",
    "            'data': data\n",
    "        }\n",
    "    \n",
    "    # ======================== REACTIVE DATA FETCHING ========================\n",
    "    \n",
    "    def get_recent_transactions(self, address: str, action: str, hours: int = 48) -> List[Dict]:\n",
    "        \"\"\"Get recent transactions (last N hours) with caching\"\"\"\n",
    "        # Check cache first\n",
    "        cached_data = self._get_cached_data(address, action)\n",
    "        if cached_data is not None:\n",
    "            print(f\"📦 Using cached {action} data for {address}\")\n",
    "            return cached_data\n",
    "        \n",
    "        # Calculate start block for recent timeframe\n",
    "        current_time = int(time.time())\n",
    "        start_time = current_time - (hours * 3600)  # N hours ago\n",
    "        \n",
    "        # Estimate block number (roughly 12 seconds per block)\n",
    "        blocks_back = hours * 300  # Approximate blocks in N hours\n",
    "        latest_block = self.get_latest_block()\n",
    "        start_block = max(0, latest_block - blocks_back)\n",
    "        \n",
    "        print(f\"🔍 Fetching recent {action} for {address} (last {hours}h, from block {start_block})\")\n",
    "        \n",
    "        # Fetch data with rate limiting\n",
    "        self._rate_limit()\n",
    "        \n",
    "        url = (\n",
    "            f\"{self.base_url}/v2/api\"\n",
    "            f\"?chainid=1\"\n",
    "            f\"&module=account\"\n",
    "            f\"&action={action}\"\n",
    "            f\"&address={address}\"\n",
    "            f\"&startblock={start_block}\"\n",
    "            f\"&endblock=latest\"\n",
    "            f\"&page=1\"\n",
    "            f\"&offset=1000\"\n",
    "            f\"&sort=desc\"\n",
    "            f\"&apikey={self.api_key}\"\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url).json()\n",
    "            txs = response.get(\"result\", [])\n",
    "            \n",
    "            if isinstance(txs, list):\n",
    "                # Filter to exact time window and cache\n",
    "                recent_txs = [tx for tx in txs if int(tx.get('timeStamp', 0)) >= start_time]\n",
    "                self._cache_data(address, action, recent_txs)\n",
    "                print(f\"✅ Fetched {len(recent_txs)} recent {action} transactions\")\n",
    "                return recent_txs\n",
    "            else:\n",
    "                print(f\"⚠️ No {action} transactions found for {address}\")\n",
    "                self._cache_data(address, action, [])\n",
    "                return []\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error fetching {action} for {address}: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def get_latest_block(self) -> int:\n",
    "        \"\"\"Get latest block number with minimal API usage\"\"\"\n",
    "        self._rate_limit()\n",
    "        url = f\"{self.base_url}/api?module=proxy&action=eth_blockNumber&apikey={self.api_key}\"\n",
    "        try:\n",
    "            response = requests.get(url).json()\n",
    "            return int(response.get('result', '0x0'), 16)\n",
    "        except:\n",
    "            return 0\n",
    "    \n",
    "    def is_contract(self, address: str) -> bool:\n",
    "        \"\"\"Check if address is contract with caching\"\"\"\n",
    "        if address in self.contract_cache:\n",
    "            return self.contract_cache[address]\n",
    "        \n",
    "        self._rate_limit()\n",
    "        url = f\"{self.base_url}/api?module=proxy&action=eth_getCode&address={address}&tag=latest&apikey={self.api_key}\"\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url).json()\n",
    "            result = response.get(\"result\", \"0x\") != \"0x\"\n",
    "            self.contract_cache[address] = result\n",
    "            return result\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    # ======================== LIGHTWEIGHT TRIGGERS ========================\n",
    "    \n",
    "    def quick_transaction_count_check(self, address: str) -> Dict:\n",
    "        \"\"\"Lightweight check - just count recent transactions without full analysis\"\"\"\n",
    "        try:\n",
    "            # Very recent check - last 1 hour only\n",
    "            recent_normal = len(self.get_recent_transactions(address, \"txlist\", hours=1))\n",
    "            recent_internal = len(self.get_recent_transactions(address, \"txlistinternal\", hours=1))\n",
    "            recent_token = len(self.get_recent_transactions(address, \"tokentx\", hours=1))\n",
    "            \n",
    "            total_recent = recent_normal + recent_internal + recent_token\n",
    "            \n",
    "            return {\n",
    "                \"address\": address,\n",
    "                \"total_1h\": total_recent,\n",
    "                \"normal_1h\": recent_normal,\n",
    "                \"internal_1h\": recent_internal,\n",
    "                \"token_1h\": recent_token,\n",
    "                \"high_activity\": total_recent > 50,  # Trigger threshold\n",
    "                \"timestamp\": datetime.now().isoformat()\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in quick check for {address}: {e}\")\n",
    "            return {\"address\": address, \"high_activity\": False}\n",
    "    \n",
    "    def add_anomaly_trigger(self, trigger_func: Callable) -> None:\n",
    "        \"\"\"Add a custom trigger function\"\"\"\n",
    "        self.anomaly_triggers.append(trigger_func)\n",
    "    \n",
    "    def add_alert_callback(self, callback: Callable) -> None:\n",
    "        \"\"\"Add callback for anomaly alerts\"\"\"\n",
    "        self.alert_callbacks.append(callback)\n",
    "    \n",
    "    # ======================== REACTIVE ANOMALY ANALYSIS ========================\n",
    "    \n",
    "    def analyze_on_trigger(self, address: str, trigger_reason: str) -> Dict:\n",
    "        \"\"\"Perform detailed analysis ONLY when triggered\"\"\"\n",
    "        print(f\"\\n🎯 TRIGGERED ANALYSIS for {address}\")\n",
    "        print(f\"📋 Reason: {trigger_reason}\")\n",
    "        \n",
    "        # Now fetch detailed data reactively\n",
    "        print(\"📡 Fetching comprehensive recent data...\")\n",
    "        \n",
    "        normal_txs = self.get_recent_transactions(address, \"txlist\", self.monitoring_window_hours)\n",
    "        internal_txs = self.get_recent_transactions(address, \"txlistinternal\", self.monitoring_window_hours)\n",
    "        token_txs = self.get_recent_transactions(address, \"tokentx\", self.monitoring_window_hours)\n",
    "        \n",
    "        # Enrich transactions\n",
    "        enriched_normal = self._enrich_transactions(normal_txs, 'normal')\n",
    "        enriched_internal = self._enrich_transactions(internal_txs, 'internal')\n",
    "        enriched_token = self._enrich_transactions(token_txs, 'token')\n",
    "        \n",
    "        # Perform detailed anomaly analysis\n",
    "        analysis_results = {\n",
    "            \"trigger_reason\": trigger_reason,\n",
    "            \"data_window\": f\"Last {self.monitoring_window_hours} hours\",\n",
    "            \"transaction_counts\": {\n",
    "                \"normal\": len(enriched_normal),\n",
    "                \"internal\": len(enriched_internal),\n",
    "                \"token\": len(enriched_token),\n",
    "                \"total\": len(enriched_normal) + len(enriched_internal) + len(enriched_token)\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Only analyze if we have sufficient data\n",
    "        if analysis_results[\"transaction_counts\"][\"total\"] >= 10:\n",
    "            all_txs = enriched_normal + enriched_internal + enriched_token\n",
    "            \n",
    "            analysis_results.update({\n",
    "                \"frequency_analysis\": self._analyze_frequency_patterns(all_txs),\n",
    "                \"bot_analysis\": self._analyze_bot_patterns(all_txs),\n",
    "                \"value_analysis\": self._analyze_value_patterns(enriched_normal + enriched_token),\n",
    "                \"gas_analysis\": self._analyze_gas_patterns(enriched_normal),\n",
    "                \"coordination_analysis\": self._analyze_coordination(enriched_normal, enriched_internal, enriched_token)\n",
    "            })\n",
    "            \n",
    "            # Calculate risk score\n",
    "            risk_assessment = self._calculate_risk_score(analysis_results)\n",
    "            analysis_results[\"risk_assessment\"] = risk_assessment\n",
    "            \n",
    "            # Trigger alerts if high risk\n",
    "            if risk_assessment[\"risk_level\"] in [\"HIGH\", \"CRITICAL\"]:\n",
    "                self._trigger_alerts(address, analysis_results)\n",
    "        \n",
    "        else:\n",
    "            analysis_results[\"insufficient_data\"] = True\n",
    "            print(\"⚠️ Insufficient recent transaction data for detailed analysis\")\n",
    "        \n",
    "        return analysis_results\n",
    "    \n",
    "    def _enrich_transactions(self, txs: List[Dict], tx_type: str) -> List[Dict]:\n",
    "        \"\"\"Enrich transactions with address types and scaled values\"\"\"\n",
    "        if not txs:\n",
    "            return []\n",
    "        \n",
    "        enriched = []\n",
    "        for tx in txs:\n",
    "            tx['tx_type'] = tx_type\n",
    "            \n",
    "            # Add address types (batch check for efficiency)\n",
    "            if tx.get('from'):\n",
    "                tx['from_type'] = \"Contract\" if self.is_contract(tx['from']) else \"EOA\"\n",
    "            if tx.get('to'):\n",
    "                tx['to_type'] = \"Contract\" if self.is_contract(tx['to']) else \"EOA\"\n",
    "            \n",
    "            # For token transactions, add scaled values\n",
    "            if tx_type == 'token' and 'tokenDecimal' in tx:\n",
    "                try:\n",
    "                    decimals = int(tx.get('tokenDecimal', 0))\n",
    "                    value = int(tx.get('value', 0))\n",
    "                    tx['value_scaled'] = value / (10 ** decimals) if decimals else value\n",
    "                except:\n",
    "                    tx['value_scaled'] = 0\n",
    "            \n",
    "            enriched.append(tx)\n",
    "        \n",
    "        return enriched\n",
    "    \n",
    "    # ======================== QUICK MONITORING SYSTEM ========================\n",
    "    \n",
    "    def start_reactive_monitoring(self, addresses: List[str]):\n",
    "        \"\"\"Start lightweight reactive monitoring\"\"\"\n",
    "        self.quick_check_addresses = set(addresses)\n",
    "        self.is_quick_monitoring = True\n",
    "        \n",
    "        def monitoring_loop():\n",
    "            print(f\"🚀 Starting reactive monitoring for {len(addresses)} addresses\")\n",
    "            print(f\"📊 Monitoring window: Last {self.monitoring_window_hours} hours\")\n",
    "            \n",
    "            while self.is_quick_monitoring:\n",
    "                for address in list(self.quick_check_addresses):\n",
    "                    try:\n",
    "                        # Lightweight check\n",
    "                        quick_result = self.quick_transaction_count_check(address)\n",
    "                        \n",
    "                        # Trigger detailed analysis if threshold exceeded\n",
    "                        if quick_result.get(\"high_activity\", False):\n",
    "                            print(f\"\\n⚡ HIGH ACTIVITY DETECTED: {address}\")\n",
    "                            print(f\"   📈 {quick_result['total_1h']} transactions in last hour\")\n",
    "                            \n",
    "                            # Reactive detailed analysis\n",
    "                            detailed_analysis = self.analyze_on_trigger(\n",
    "                                address, \n",
    "                                f\"High activity: {quick_result['total_1h']} txs/hour\"\n",
    "                            )\n",
    "                        \n",
    "                        # Custom triggers\n",
    "                        for trigger_func in self.anomaly_triggers:\n",
    "                            try:\n",
    "                                trigger_result = trigger_func(address, quick_result)\n",
    "                                if trigger_result.get(\"triggered\", False):\n",
    "                                    self.analyze_on_trigger(address, trigger_result[\"reason\"])\n",
    "                            except Exception as e:\n",
    "                                print(f\"Error in custom trigger: {e}\")\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"Error in monitoring {address}: {e}\")\n",
    "                    \n",
    "                    # Rate limiting between addresses\n",
    "                    time.sleep(10)  # 10 seconds between address checks\n",
    "                \n",
    "                # Wait before next monitoring cycle\n",
    "                print(f\"💤 Monitoring cycle complete. Next check in 5 minutes...\")\n",
    "                time.sleep(300)  # 5 minute cycles for lightweight checks\n",
    "        \n",
    "        self.quick_monitor_thread = threading.Thread(target=monitoring_loop, daemon=True)\n",
    "        self.quick_monitor_thread.start()\n",
    "    \n",
    "    def stop_reactive_monitoring(self):\n",
    "        \"\"\"Stop reactive monitoring\"\"\"\n",
    "        self.is_quick_monitoring = False\n",
    "        print(\"🛑 Reactive monitoring stopped\")\n",
    "    \n",
    "    # ======================== ANALYSIS METHODS (Simplified) ========================\n",
    "    \n",
    "    def _analyze_frequency_patterns(self, txs: List[Dict]) -> Dict:\n",
    "        \"\"\"Analyze transaction frequency patterns\"\"\"\n",
    "        if len(txs) < 5:\n",
    "            return {\"insufficient_data\": True}\n",
    "        \n",
    "        timestamps = [int(tx.get('timeStamp', 0)) for tx in txs]\n",
    "        timestamps.sort()\n",
    "        \n",
    "        # Check for bursts (>10 txs in 10 minutes)\n",
    "        burst_windows = []\n",
    "        window_size = 600  # 10 minutes\n",
    "        \n",
    "        for i, ts in enumerate(timestamps):\n",
    "            window_end = ts + window_size\n",
    "            count_in_window = sum(1 for t in timestamps[i:] if t <= window_end)\n",
    "            \n",
    "            if count_in_window > 10:\n",
    "                burst_windows.append({\"start\": ts, \"count\": count_in_window})\n",
    "        \n",
    "        return {\n",
    "            \"total_transactions\": len(txs),\n",
    "            \"time_span_hours\": (max(timestamps) - min(timestamps)) / 3600,\n",
    "            \"burst_windows\": len(burst_windows),\n",
    "            \"high_frequency\": len(burst_windows) > 0\n",
    "        }\n",
    "    \n",
    "    def _analyze_bot_patterns(self, txs: List[Dict]) -> Dict:\n",
    "        \"\"\"Analyze for bot-like patterns\"\"\"\n",
    "        if len(txs) < 10:\n",
    "            return {\"insufficient_data\": True}\n",
    "        \n",
    "        timestamps = sorted([int(tx.get('timeStamp', 0)) for tx in txs])\n",
    "        intervals = [timestamps[i+1] - timestamps[i] for i in range(len(timestamps)-1)]\n",
    "        \n",
    "        # Look for regular intervals\n",
    "        regular_10s = sum(1 for i in intervals if 8 <= i <= 12)\n",
    "        regular_60s = sum(1 for i in intervals if 58 <= i <= 62)\n",
    "        \n",
    "        return {\n",
    "            \"regular_10s_intervals\": regular_10s,\n",
    "            \"regular_60s_intervals\": regular_60s,\n",
    "            \"total_intervals\": len(intervals),\n",
    "            \"bot_suspected\": (regular_10s > len(intervals) * 0.7) or (regular_60s > len(intervals) * 0.7)\n",
    "        }\n",
    "    \n",
    "    def _analyze_value_patterns(self, txs: List[Dict]) -> Dict:\n",
    "        \"\"\"Analyze value transfer patterns\"\"\"\n",
    "        values = []\n",
    "        for tx in txs:\n",
    "            if 'value_scaled' in tx and tx['value_scaled'] > 0:\n",
    "                values.append(tx['value_scaled'])\n",
    "            elif int(tx.get('value', 0)) > 0:\n",
    "                values.append(int(tx['value']))\n",
    "        \n",
    "        if not values:\n",
    "            return {\"no_value_transfers\": True}\n",
    "        \n",
    "        # Check for round numbers and suspicious patterns\n",
    "        round_values = sum(1 for v in values if v == int(v) and v > 0)\n",
    "        \n",
    "        return {\n",
    "            \"total_value_transfers\": len(values),\n",
    "            \"round_number_ratio\": round_values / len(values),\n",
    "            \"suspicious_round_numbers\": round_values / len(values) > 0.8\n",
    "        }\n",
    "    \n",
    "    def _analyze_gas_patterns(self, normal_txs: List[Dict]) -> Dict:\n",
    "        \"\"\"Analyze gas usage patterns\"\"\"\n",
    "        gas_prices = [int(tx.get('gasPrice', 0)) for tx in normal_txs if tx.get('gasPrice')]\n",
    "        \n",
    "        if not gas_prices:\n",
    "            return {\"no_gas_data\": True}\n",
    "        \n",
    "        avg_gas = sum(gas_prices) / len(gas_prices)\n",
    "        high_gas_count = sum(1 for g in gas_prices if g > avg_gas * 5)\n",
    "        \n",
    "        return {\n",
    "            \"avg_gas_price\": avg_gas,\n",
    "            \"high_gas_transactions\": high_gas_count,\n",
    "            \"suspicious_gas_usage\": high_gas_count > len(gas_prices) * 0.1\n",
    "        }\n",
    "    \n",
    "    def _analyze_coordination(self, normal_txs: List[Dict], internal_txs: List[Dict], token_txs: List[Dict]) -> Dict:\n",
    "        \"\"\"Analyze coordination across transaction types\"\"\"\n",
    "        timestamp_types = defaultdict(set)\n",
    "        \n",
    "        for tx_list, tx_type in [(normal_txs, 'normal'), (internal_txs, 'internal'), (token_txs, 'token')]:\n",
    "            for tx in tx_list:\n",
    "                ts = tx.get('timeStamp', '')\n",
    "                if ts:\n",
    "                    timestamp_types[ts].add(tx_type)\n",
    "        \n",
    "        coordinated_timestamps = sum(1 for types in timestamp_types.values() if len(types) > 1)\n",
    "        \n",
    "        return {\n",
    "            \"coordinated_timestamps\": coordinated_timestamps,\n",
    "            \"total_timestamps\": len(timestamp_types),\n",
    "            \"coordination_ratio\": coordinated_timestamps / len(timestamp_types) if timestamp_types else 0,\n",
    "            \"high_coordination\": coordinated_timestamps / len(timestamp_types) > 0.3 if timestamp_types else False\n",
    "        }\n",
    "    \n",
    "    def _calculate_risk_score(self, analysis: Dict) -> Dict:\n",
    "        \"\"\"Calculate overall risk score\"\"\"\n",
    "        score = 0\n",
    "        flags = []\n",
    "        \n",
    "        # High frequency activity\n",
    "        if analysis.get(\"frequency_analysis\", {}).get(\"high_frequency\", False):\n",
    "            score += 25\n",
    "            flags.append(\"High Frequency Bursts\")\n",
    "        \n",
    "        # Bot patterns\n",
    "        if analysis.get(\"bot_analysis\", {}).get(\"bot_suspected\", False):\n",
    "            score += 35\n",
    "            flags.append(\"Bot-like Patterns\")\n",
    "        \n",
    "        # Suspicious values\n",
    "        if analysis.get(\"value_analysis\", {}).get(\"suspicious_round_numbers\", False):\n",
    "            score += 20\n",
    "            flags.append(\"Suspicious Value Patterns\")\n",
    "        \n",
    "        # Gas manipulation\n",
    "        if analysis.get(\"gas_analysis\", {}).get(\"suspicious_gas_usage\", False):\n",
    "            score += 30\n",
    "            flags.append(\"Gas Price Manipulation\")\n",
    "        \n",
    "        # Coordination\n",
    "        if analysis.get(\"coordination_analysis\", {}).get(\"high_coordination\", False):\n",
    "            score += 40\n",
    "            flags.append(\"Multi-Type Coordination\")\n",
    "        \n",
    "        # Risk levels\n",
    "        if score >= 80:\n",
    "            risk_level = \"CRITICAL\"\n",
    "        elif score >= 60:\n",
    "            risk_level = \"HIGH\"\n",
    "        elif score >= 30:\n",
    "            risk_level = \"MEDIUM\"\n",
    "        else:\n",
    "            risk_level = \"LOW\"\n",
    "        \n",
    "        return {\n",
    "            \"risk_score\": min(score, 100),\n",
    "            \"risk_level\": risk_level,\n",
    "            \"flags\": flags\n",
    "        }\n",
    "    \n",
    "    def _trigger_alerts(self, address: str, analysis: Dict):\n",
    "        \"\"\"Trigger alert callbacks\"\"\"\n",
    "        alert_data = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"address\": address,\n",
    "            \"analysis\": analysis\n",
    "        }\n",
    "        \n",
    "        for callback in self.alert_callbacks:\n",
    "            try:\n",
    "                callback(alert_data)\n",
    "            except Exception as e:\n",
    "                print(f\"Error in alert callback: {e}\")\n",
    "\n",
    "# ======================== USAGE EXAMPLE ========================\n",
    "\n",
    "def reactive_alert_callback(alert_data):\n",
    "    \"\"\"Callback for reactive alerts\"\"\"\n",
    "    address = alert_data['address']\n",
    "    analysis = alert_data['analysis']\n",
    "    risk = analysis['risk_assessment']\n",
    "    \n",
    "    print(f\"\\n🚨 REACTIVE ANOMALY DETECTED!\")\n",
    "    print(f\"📍 Address: {address}\")\n",
    "    print(f\"⏰ Time: {alert_data['timestamp']}\")\n",
    "    print(f\"🎯 Trigger: {analysis['trigger_reason']}\")\n",
    "    print(f\"📊 Risk Level: {risk['risk_level']} ({risk['risk_score']}/100)\")\n",
    "    print(f\"🚩 Flags: {', '.join(risk['flags'])}\")\n",
    "    print(f\"📈 Transactions: {analysis['transaction_counts']}\")\n",
    "\n",
    "def custom_trigger_example(address: str, quick_check: Dict) -> Dict:\n",
    "    \"\"\"Example custom trigger function\"\"\"\n",
    "    # Trigger if >30 token transactions in 1 hour\n",
    "    if quick_check.get(\"token_1h\", 0) > 30:\n",
    "        return {\n",
    "            \"triggered\": True,\n",
    "            \"reason\": f\"High token activity: {quick_check['token_1h']} token txs/hour\"\n",
    "        }\n",
    "    return {\"triggered\": False}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize reactive detector\n",
    "    API_KEY = \"YOUR_ETHERSCAN_API_KEY\"\n",
    "    detector = ReactiveEtherscanAnomalyDetector(API_KEY)\n",
    "    \n",
    "    # Add alert callback\n",
    "    detector.add_alert_callback(reactive_alert_callback)\n",
    "    \n",
    "    # Add custom triggers\n",
    "    detector.add_anomaly_trigger(custom_trigger_example)\n",
    "    \n",
    "    # Start reactive monitoring (lightweight)\n",
    "    addresses_to_monitor = [\n",
    "        \"0xE592427A0AEce92De3Edee1F18E0157C05861564\",  # Uniswap V3\n",
    "        \"0x7a250d5630B4cF539739dF2C5dAcb4c659F2488D\",  # Uniswap V2\n",
    "    ]\n",
    "    \n",
    "    detector.start_reactive_monitoring(addresses_to_monitor)\n",
    "    \n",
    "    print(f\"🎯 REACTIVE MONITORING ACTIVE\")\n",
    "    print(f\"📊 Monitoring {len(addresses_to_monitor)} addresses\")\n",
    "    print(f\"⚡ Will fetch detailed data ONLY when anomalies detected\")\n",
    "    print(f\"🔄 Lightweight checks every 5 minutes\")\n",
    "    print(f\"📅 Analysis window: Last 48 hours\")\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            time.sleep(300)  # Check status every 5 minutes\n",
    "            print(f\"📡 Cache entries: {len(detector.address_cache)}\")\n",
    "            print(f\"🔢 Daily API calls: {detector.daily_request_count}\")\n",
    "    except KeyboardInterrupt:\n",
    "        detector.stop_reactive_monitoring()\n",
    "        print(\"\\n✅ Reactive monitoring stopped\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
